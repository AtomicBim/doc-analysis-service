"""
FastAPI —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:
- –¢–ó/–¢–£: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
- –ß–µ—Ä—Ç–µ–∂–∏: OpenAI Assistants API —Å File Search –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
"""
import os
import json
import logging
import asyncio
import warnings
from typing import List, Optional, Dict, Any
from pathlib import Path
import fitz  # pymupdf
from tenacity import retry, stop_after_attempt, wait_exponential

# –û—Ç–∫–ª—é—á–∞–µ–º warnings –æ deprecation
warnings.filterwarnings("ignore", category=DeprecationWarning, module="openai")

import uvicorn
from fastapi import FastAPI, HTTPException, Form, UploadFile, File, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError
from openai import AsyncOpenAI
from dotenv import load_dotenv

# ============================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è!")
    raise ValueError("OPENAI_API_KEY is required")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.1"))
MAX_FILE_SIZE_MB = 40
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024

logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI API (HYBRID MODE): {OPENAI_MODEL}")
logger.info("üìã –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –¢–ó/–¢–£ –ø–∞—Ä—Å–∏–Ω–≥ –≤—Ä—É—á–Ω—É—é + –ß–µ—Ä—Ç–µ–∂–∏ —á–µ—Ä–µ–∑ Assistants API")


# ============================
# –°–ò–°–¢–ï–ú–ê –ü–†–û–ú–ü–¢–û–í
# ============================

def load_prompts() -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ prompts."""
    prompts = {}
    prompts_dir = Path(__file__).parent.parent / "prompts"

    stage_files = {
        "–ì–ö": "gk_prompt.txt",
        "–§–≠": "fe_prompt.txt",
        "–≠–ü": "ep_prompt.txt"
    }

    for stage, filename in stage_files.items():
        file_path = prompts_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                prompts[stage] = f.read().strip()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage}")
        except FileNotFoundError:
            logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞: {file_path}")
            raise FileNotFoundError(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

    return prompts


def load_tu_prompts() -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¢–£ –¥–ª—è —Å—Ç–∞–¥–∏–π –§–≠ –∏ –≠–ü."""
    tu_prompts: Dict[str, str] = {}
    prompts_dir = Path(__file__).parent.parent / "prompts"

    tu_stage_files = {
        "–§–≠": "tu_fe.txt",
        "–≠–ü": "tu_ep.txt",
    }

    for stage, filename in tu_stage_files.items():
        file_path = prompts_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                tu_prompts[stage] = f.read().strip()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¢–£ –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage}")
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¢–£: {file_path}")
            tu_prompts[stage] = ""

    return tu_prompts

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
PROMPTS = load_prompts()
TU_PROMPTS = load_tu_prompts()

# ============================
# ASSISTANTS API –§–£–ù–ö–¶–ò–ò
# ============================

async def upload_to_vector_store(doc_content: bytes, filename: str) -> str:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —á–µ—Ä—Ç–µ–∂–µ–π –≤ Vector Store –¥–ª—è File Search.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç vector_store_id.
    """
    logger.info(f"üì§ –°–æ–∑–¥–∞–Ω–∏–µ Vector Store –¥–ª—è {filename}...")

    # –°–æ–∑–¥–∞–µ–º Vector Store —Å –∞–≤—Ç–æ—É–¥–∞–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ 1 –¥–µ–Ω—å
    vector_store = await client.beta.vector_stores.create(
        name=f"Project Documentation - {filename}",
        expires_after={"anchor": "last_active_at", "days": 1}
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    temp_file_path = f"/tmp/{filename}"
    with open(temp_file_path, 'wb') as f:
        f.write(doc_content)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –≤ Vector Store
    try:
        with open(temp_file_path, 'rb') as f:
            file_batch = await client.beta.vector_stores.file_batches.upload_and_poll(
                vector_store_id=vector_store.id,
                files=[f]
            )

        logger.info(f"‚úÖ Vector Store —Å–æ–∑–¥–∞–Ω: {vector_store.id}, —Å—Ç–∞—Ç—É—Å: {file_batch.status}")
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

    return vector_store.id


async def create_analysis_assistant(stage: str, req_type: str) -> str:
    """
    –°–æ–∑–¥–∞–µ—Ç Assistant –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç assistant_id.
    """
    stage_prompt = PROMPTS.get(stage, PROMPTS["–§–≠"])

    instructions = f"""{stage_prompt}

–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞:

1. –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ {req_type}
2. –ù–∞–π—Ç–∏ –≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (—á–µ—Ä—Ç–µ–∂–∞—Ö) —Ä–µ—à–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
3. –í–µ—Ä–Ω—É—Ç—å –∞–Ω–∞–ª–∏–∑ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:

{{
  "number": <–Ω–æ–º–µ—Ä —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è>,
  "requirement": "<—Ç–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è>",
  "status": "<–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è>",
  "confidence": <0-100>,
  "solution_description": "<–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ>",
  "reference": "<–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞: –Ω–æ–º–µ—Ä –ª–∏—Å—Ç–∞, —Ä–∞–∑–¥–µ–ª, —Å—Ç—Ä–∞–Ω–∏—Ü–∞>",
  "discrepancies": "<–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ '-'>",
  "recommendations": "<—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–ª–∏ '-'>"
}}

–í–ê–ñ–ù–û:
- –ò—Å–ø–æ–ª—å–∑—É–π File Search –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π —á–µ—Ä—Ç–µ–∂–µ–π
- –£–∫–∞–∑—ã–≤–∞–π –ö–û–ù–ö–†–ï–¢–ù–´–ï —Å—Å—ã–ª–∫–∏ (–Ω–æ–º–µ—Ä–∞ –ª–∏—Å—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—ã, —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∫ —Ç–µ–∫—Å—Ç, —Ç–∞–∫ –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂–∞—Ö
- –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —É–∫–∞–∑—ã–≤–∞–π status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è"
- –í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π
"""

    assistant = await client.beta.assistants.create(
        name=f"Document Analyzer - {stage}",
        instructions=instructions,
        model=OPENAI_MODEL,
        tools=[{"type": "file_search"}],
        temperature=TEMPERATURE
    )

    logger.info(f"ü§ñ Assistant —Å–æ–∑–¥–∞–Ω: {assistant.id}")
    return assistant.id


async def analyze_requirement_with_assistant(
    assistant_id: str,
    vector_store_id: str,
    requirement: Dict[str, Any],
    request: Request
) -> Optional['RequirementAnalysis']:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Assistants API —Å File Search.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç RequirementAnalysis –∏–ª–∏ None –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞.
    """
    if await request.is_disconnected():
        logger.warning(f"‚ö†Ô∏è Client disconnected before analyzing {requirement['trace_id']}")
        return None

    logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è {requirement['trace_id']}...")

    # –°–æ–∑–¥–∞–µ–º Thread
    thread = await client.beta.threads.create()

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
    await client.beta.threads.messages.create(
        thread_id=thread.id,
        role="user",
        content=f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–µ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¢–ó:

–ù–æ–º–µ—Ä: {requirement.get('number')}
–†–∞–∑–¥–µ–ª: {requirement.get('section', '–û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è')}
–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: {requirement['text']}

–ù–∞–π–¥–∏ –≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (—á–µ—Ä—Ç–µ–∂–∞—Ö), –∫–∞–∫ —ç—Ç–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.
–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º Assistant —Å File Search
    run = await client.beta.threads.runs.create_and_poll(
        thread_id=thread.id,
        assistant_id=assistant_id,
        tool_resources={"file_search": {"vector_store_ids": [vector_store_id]}},
        timeout=300  # 5 –º–∏–Ω—É—Ç –Ω–∞ –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    )

    if run.status != 'completed':
        logger.error(f"‚ùå Run failed for {requirement['trace_id']}: {run.status}")
        return RequirementAnalysis(
            number=requirement.get('number', 0),
            requirement=requirement['text'],
            status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
            confidence=0,
            solution_description="–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
            reference="-",
            discrepancies=f"Assistant run status: {run.status}",
            recommendations="–ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∞–Ω–∞–ª–∏–∑",
            section=requirement.get('section'),
            trace_id=requirement['trace_id']
        )

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
    messages = await client.beta.threads.messages.list(thread_id=thread.id, order="desc", limit=1)
    assistant_message = messages.data[0]

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
    response_text = ""
    for content_block in assistant_message.content:
        if content_block.type == 'text':
            response_text += content_block.text.value

    # –ò–∑–≤–ª–µ–∫–∞–µ–º citations –¥–ª—è —Å—Å—ã–ª–æ–∫
    citations = []
    if hasattr(assistant_message.content[0], 'text') and hasattr(assistant_message.content[0].text, 'annotations'):
        for annotation in assistant_message.content[0].text.annotations:
            if hasattr(annotation, 'file_citation'):
                citations.append({
                    'file_id': annotation.file_citation.file_id,
                    'quote': annotation.file_citation.quote
                })

    # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
    try:
        # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            data = json.loads(json_str)

            # –î–æ–±–∞–≤–ª—è–µ–º citations –≤ reference –µ—Å–ª–∏ –µ—Å—Ç—å
            if citations:
                citation_text = " | ".join([f"–¶–∏—Ç–∞—Ç–∞: {c['quote'][:100]}..." for c in citations[:2]])
                data['reference'] = f"{data.get('reference', '-')} {citation_text}"

            return RequirementAnalysis(
                **data,
                section=requirement.get('section'),
                trace_id=requirement['trace_id']
            )
        else:
            raise ValueError("No JSON found in response")
    except (json.JSONDecodeError, ValidationError) as e:
        logger.error(f"‚ùå Failed to parse assistant response for {requirement['trace_id']}: {e}")
        logger.error(f"Response was: {response_text[:500]}")
        return RequirementAnalysis(
            number=requirement.get('number', 0),
            requirement=requirement['text'],
            status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
            confidence=50,
            solution_description=response_text[:200] if response_text else "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç",
            reference="; ".join([c['quote'][:50] for c in citations]) if citations else "-",
            discrepancies="–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
            recommendations="–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é",
            section=requirement.get('section'),
            trace_id=requirement['trace_id']
        )


async def cleanup_assistant_resources(assistant_id: Optional[str], vector_store_id: Optional[str]):
    """–£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã Assistants API."""
    try:
        if assistant_id:
            await client.beta.assistants.delete(assistant_id)
            logger.info(f"üóëÔ∏è Assistant —É–¥–∞–ª–µ–Ω: {assistant_id}")
        if vector_store_id:
            await client.beta.vector_stores.delete(vector_store_id)
            logger.info(f"üóëÔ∏è Vector Store —É–¥–∞–ª–µ–Ω: {vector_store_id}")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Ä–µ—Å—É—Ä—Å–æ–≤: {e}")


# ============================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –¢–ó/–¢–£
# ============================

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def extract_text_from_pdf(content: bytes, filename: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OCR –µ—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è."""
    import base64
    from PIL import Image
    import io

    text = ""
    doc = fitz.open(stream=content, filetype="pdf")
    is_scanned = True

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
    for page in doc:
        page_text = page.get_text()
        if page_text.strip():
            is_scanned = False
            text += page_text + "\n\n"

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR —á–µ—Ä–µ–∑ OpenAI Vision
    if is_scanned or not text.strip():
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {filename} –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω, –ø—Ä–∏–º–µ–Ω—è–µ–º OCR —á–µ—Ä–µ–∑ OpenAI Vision...")

        for page_num, page in enumerate(doc):
            # –†–µ–Ω–¥–µ—Ä–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            pix = page.get_pixmap(dpi=150)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG', quality=85)
            base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')

            # OCR —á–µ—Ä–µ–∑ Vision
            logger.info(f"üìÑ OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}/{len(doc)} –∏–∑ {filename}")
            response = await client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–æ–≤, —Ç–∞–±–ª–∏—Ü—ã. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                            }
                        ]
                    }
                ],
                temperature=0.0,
                max_tokens=4000
            )

            page_text = response.choices[0].message.content
            text += f"\n\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n\n{page_text}"

        logger.info(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {filename}, –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

    doc.close()

    if not text.strip():
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {filename}")
        return "[–î–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç]"

    return text.strip()


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def segment_requirements(tz_text: str) -> List[Dict[str, Any]]:
    """–°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –¢–ó –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è GPT."""
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –¢–ó –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–∑ –Ω–µ–≥–æ —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.

–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∂–∏:
- number: –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ)
- text: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- section: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
- trace_id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'req-{{number}}'

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{"requirements": [{{"number": 1, "text": "...", "section": "...", "trace_id": "req-1"}}]}}

–¢–µ–∫—Å—Ç –¢–ó:
{tz_text[:10000]}"""  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10000 —Å–∏–º–≤–æ–ª–æ–≤

    response = await client.chat.completions.create(
        model="gpt-4o-mini",  # –î–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        messages=[{"role": "user", "content": prompt}],
        temperature=TEMPERATURE,
        response_format={"type": "json_object"}
    )

    try:
        data = json.loads(response.choices[0].message.content)
        requirements = data.get("requirements", [])
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(requirements)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π")
        return requirements
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Failed to parse requirements JSON: {e}")
        raise ValueError("Failed to parse requirements JSON")


async def _get_file_size(file: UploadFile) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö."""
    content = await file.read()
    await file.seek(0)
    return len(content)


# ============================
# PYDANTIC –ú–û–î–ï–õ–ò
# ============================

class RequirementAnalysis(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"""
    number: int
    requirement: str
    status: str
    confidence: int
    solution_description: str
    reference: str
    discrepancies: str
    recommendations: str
    section: Optional[str] = None
    trace_id: Optional[str] = None


class AnalysisResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    stage: str
    req_type: str
    requirements: List[RequirementAnalysis]
    summary: str


# ============================
# FASTAPI –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ============================

app = FastAPI(
    title="Document Analysis API (Hybrid)",
    description="API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞",
    version="5.0.0-hybrid"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================
# API ENDPOINTS
# ============================

@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "ok",
        "service": "Document Analysis API (HYBRID)",
        "architecture": "TZ/TU manual parsing + Drawings via Assistants API",
        "provider": "openai",
        "model": OPENAI_MODEL,
        "max_file_size_mb": MAX_FILE_SIZE_MB
    }


@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_documentation(
    request: Request,
    stage: str = Form(...),
    check_tu: bool = Form(False),
    req_type: str = Form("–¢–ó"),
    tz_document: UploadFile = File(...),
    doc_document: UploadFile = File(...),
    tu_document: Optional[UploadFile] = File(None)
):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
    - –¢–ó/–¢–£: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    - –ß–µ—Ä—Ç–µ–∂–∏: Assistants API —Å File Search
    """
    assistant_id = None
    vector_store_id = None

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–∫–ª—é—á–∏–ª—Å—è –ª–∏ –∫–ª–∏–µ–Ω—Ç
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected before analysis started. Aborting.")
            raise HTTPException(status_code=499, detail="Client disconnected")

        logger.info(f"üìã [HYBRID] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑. –°—Ç–∞–¥–∏—è: {stage}, check_tu: {check_tu}")

        # ============================================================
        # –≠–¢–ê–ü 1: –ü–∞—Ä—Å–∏–Ω–≥ –¢–ó/–¢–£ (—Ä—É—á–Ω–æ–π, –±—ã—Å—Ç—Ä—ã–π, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π)
        # ============================================================

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        files_to_check = [tz_document, doc_document]
        if tu_document:
            files_to_check.append(tu_document)

        for file in files_to_check:
            file_size = await _get_file_size(file)
            if file_size > MAX_FILE_SIZE_BYTES:
                raise HTTPException(
                    status_code=413,
                    detail=f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file_size / 1024 / 1024:.2f} MB). –ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB} MB"
                )

        # Read contents
        await tz_document.seek(0)
        await doc_document.seek(0)
        if tu_document:
            await tu_document.seek(0)

        tz_content = await tz_document.read()
        doc_content = await doc_document.read()
        tu_content = None
        if tu_document:
            tu_content = await tu_document.read()

        logger.info(f"üìä File sizes - TZ: {len(tz_content) / 1024:.1f} KB, DOC: {len(doc_content) / 1024:.1f} KB")

        # Extract TZ text (—Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥)
        logger.info("üìÑ [STEP 1/4] Extracting text from TZ...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected during TZ extraction")
            return {"error": "Client disconnected"}

        tz_text = await extract_text_from_pdf(tz_content, tz_document.filename)

        # Handle TU if needed
        has_tu = check_tu and (tu_content is not None or stage in TU_PROMPTS)
        if has_tu:
            logger.info("üìÑ Adding TU to requirements...")
            tu_text = await extract_text_from_pdf(tu_content, tu_document.filename) if tu_content else TU_PROMPTS.get(stage, "")
            tz_text += "\n\n=== –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è (–¢–£) ===\n" + tu_text

        # Segment requirements (–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è)
        logger.info("‚úÇÔ∏è [STEP 2/4] Segmenting requirements from TZ/TU...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected during segmentation")
            return {"error": "Client disconnected"}

        requirements = await segment_requirements(tz_text)

        if not requirements:
            raise HTTPException(status_code=400, detail="No requirements extracted from TZ")

        logger.info(f"‚úÖ Extracted {len(requirements)} requirements")

        # ============================================================
        # –≠–¢–ê–ü 2: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä—Ç–µ–∂–µ–π –≤ Assistants API Vector Store
        # ============================================================

        logger.info("üì§ [STEP 3/4] Uploading project documentation to Vector Store...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected before upload")
            return {"error": "Client disconnected"}

        vector_store_id = await upload_to_vector_store(doc_content, doc_document.filename)

        # ============================================================
        # –≠–¢–ê–ü 3: –°–æ–∑–¥–∞–Ω–∏–µ Assistant –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        # ============================================================

        logger.info("ü§ñ Creating Assistant for analysis...")
        assistant_id = await create_analysis_assistant(stage, "–¢–ó+–¢–£" if has_tu else "–¢–ó")

        # ============================================================
        # –≠–¢–ê–ü 4: –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ Assistant
        # ============================================================

        logger.info(f"üîç [STEP 4/4] Analyzing {len(requirements)} requirements with Assistants API...")
        analyzed_reqs = []

        for idx, req in enumerate(requirements, 1):
            if await request.is_disconnected():
                logger.warning(f"‚ö†Ô∏è Client disconnected at requirement {idx}/{len(requirements)}")
                await cleanup_assistant_resources(assistant_id, vector_store_id)
                return {"error": "Client disconnected"}

            logger.info(f"üîç [{idx}/{len(requirements)}] Analyzing: {req.get('trace_id')}")

            result = await analyze_requirement_with_assistant(
                assistant_id=assistant_id,
                vector_store_id=vector_store_id,
                requirement=req,
                request=request
            )

            if result is None:  # Client disconnected
                await cleanup_assistant_resources(assistant_id, vector_store_id)
                return {"error": "Client disconnected"}

            analyzed_reqs.append(result)

        # ============================================================
        # –≠–¢–ê–ü 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏
        # ============================================================

        logger.info("üìù Generating summary...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected before summary")
            await cleanup_assistant_resources(assistant_id, vector_store_id)
            return {"error": "Client disconnected"}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Å–≤–æ–¥–∫–∏
        total = len(analyzed_reqs)
        completed = sum(1 for r in analyzed_reqs if r.status == "–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ")
        partial = sum(1 for r in analyzed_reqs if r.status == "–ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ")
        not_done = sum(1 for r in analyzed_reqs if r.status == "–ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ")
        unclear = sum(1 for r in analyzed_reqs if r.status == "–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è")

        summary = f"""–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {total}
- –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ: {completed} ({completed/total*100:.1f}%)
- –ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ: {partial} ({partial/total*100:.1f}%)
- –ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ: {not_done} ({not_done/total*100:.1f}%)
- –¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è: {unclear} ({unclear/total*100:.1f}%)

–°—Ä–µ–¥–Ω—è—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {sum(r.confidence for r in analyzed_reqs)/total:.1f}%"""

        # ============================================================
        # Cleanup –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        # ============================================================

        await cleanup_assistant_resources(assistant_id, vector_store_id)

        parsed_result = AnalysisResponse(
            stage=stage,
            req_type="–¢–ó+–¢–£" if has_tu else "–¢–ó",
            requirements=analyzed_reqs,
            summary=summary
        )

        logger.info(f"‚úÖ [HYBRID] –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyzed_reqs)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.")
        return parsed_result

    except HTTPException:
        await cleanup_assistant_resources(assistant_id, vector_store_id)
        raise
    except Exception as e:
        logger.error(f"‚ùå [HYBRID] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
        await cleanup_assistant_resources(assistant_id, vector_store_id)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


# ============================
# –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê
# ============================

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
