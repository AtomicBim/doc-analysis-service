"""
FastAPI —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:
- –¢–ó/–¢–£: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
- –ß–µ—Ä—Ç–µ–∂–∏: OpenAI Assistants API —Å File Search –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
"""
import os
import json
import logging
import asyncio
import warnings
from typing import List, Optional, Dict, Any
from pathlib import Path
import fitz  # pymupdf
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from openai import RateLimitError

# –û—Ç–∫–ª—é—á–∞–µ–º warnings –æ deprecation
warnings.filterwarnings("ignore", category=DeprecationWarning, module="openai")

import uvicorn
from fastapi import FastAPI, HTTPException, Form, UploadFile, File, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError
from openai import AsyncOpenAI
from dotenv import load_dotenv
from config import (
    # Stage 1
    STAGE1_MAX_PAGES, STAGE1_DPI, STAGE1_QUALITY, STAGE1_MAX_PAGES_PER_REQUEST,
    STAGE1_STAMP_CROP, STAGE1_TOP_RIGHT_CROP, STAGE1_HEADER_CROP,
    # Stage 2
    STAGE2_MAX_PAGES, STAGE2_DPI, STAGE2_QUALITY, STAGE2_DETAIL, STAGE2_MAX_PAGES_PER_REQUEST,
    # Stage 3
    STAGE3_DPI, STAGE3_QUALITY, STAGE3_DETAIL, STAGE3_BATCH_SIZE, STAGE3_MAX_TOKENS, STAGE3_RETRY_ON_REFUSAL, STAGE3_MAX_PAGES_PER_REQUEST,
    # Stage 4
    STAGE4_ENABLED, STAGE4_SAMPLE_PAGES_PER_SECTION, STAGE4_DPI, STAGE4_QUALITY, STAGE4_DETAIL, STAGE4_MAX_TOKENS,
    # Retry
    RETRY_MAX_ATTEMPTS, RETRY_WAIT_EXPONENTIAL_MULTIPLIER, RETRY_WAIT_EXPONENTIAL_MAX,
    # OpenAI
    OPENAI_MODEL, OPENAI_TEMPERATURE,
    # Logging
    LOG_LEVEL, LOG_RESPONSE_PREVIEW_LENGTH, LOG_FULL_RESPONSE_ON_ERROR
)

# ============================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================

logging.basicConfig(level=getattr(logging, LOG_LEVEL), format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è!")
    raise ValueError("OPENAI_API_KEY is required")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)
TEMPERATURE = OPENAI_TEMPERATURE
MAX_FILE_SIZE_MB = 40
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024

logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI API (VISION MODE): {OPENAI_MODEL}")
logger.info("üìã –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –¢–ó/–¢–£ –ø–∞—Ä—Å–∏–Ω–≥ –≤—Ä—É—á–Ω—É—é + –ß–µ—Ä—Ç–µ–∂–∏ —á–µ—Ä–µ–∑ Vision API")


# ============================
# –°–ò–°–¢–ï–ú–ê –ü–†–û–ú–ü–¢–û–í
# ============================

def load_prompts() -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ prompts."""
    prompts = {}
    prompts_dir = Path(__file__).parent.parent / "prompts"

    stage_files = {
        "–ì–ö": "gk_prompt.txt",
        "–§–≠": "fe_prompt.txt",
        "–≠–ü": "ep_prompt.txt"
    }

    for stage, filename in stage_files.items():
        file_path = prompts_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                prompts[stage] = f.read().strip()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage}")
        except FileNotFoundError:
            logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞: {file_path}")
            raise FileNotFoundError(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

    return prompts


def load_tu_prompts() -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¢–£ –¥–ª—è —Å—Ç–∞–¥–∏–π –§–≠ –∏ –≠–ü."""
    tu_prompts: Dict[str, str] = {}
    prompts_dir = Path(__file__).parent.parent / "prompts"

    tu_stage_files = {
        "–§–≠": "tu_fe.txt",
        "–≠–ü": "tu_ep.txt",
    }

    for stage, filename in tu_stage_files.items():
        file_path = prompts_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                tu_prompts[stage] = f.read().strip()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¢–£ –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage}")
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¢–£: {file_path}")
            tu_prompts[stage] = ""

    return tu_prompts

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
PROMPTS = load_prompts()
TU_PROMPTS = load_tu_prompts()

# ============================
# PDF PROCESSING –§–£–ù–ö–¶–ò–ò
# ============================

async def extract_pdf_pages_as_images(doc_content: bytes, filename: str, max_pages: int = 150, detail: str = "low", dpi: int = 100, quality: int = 70) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF –∫–∞–∫ base64-encoded –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Vision API.

    Args:
        detail: "low" (85 tokens/img) –∏–ª–∏ "high" (765 tokens/img)
        dpi: –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ (100 –¥–ª—è low, 150 –¥–ª—è high)
        quality: JPEG –∫–∞—á–µ—Å—Ç–≤–æ (70 –¥–ª—è low, 85 –¥–ª—è high)
    """
    logger.info(f"üìÑ [IMG] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ {filename} (detail={detail}, dpi={dpi}, quality={quality})...")

    def _extract():
        import base64
        from PIL import Image
        import io

        doc = fitz.open(stream=doc_content, filetype="pdf")
        images = []

        total_pages = min(len(doc), max_pages)
        logger.info(f"üìÑ [IMG] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ {len(doc)}")

        for page_num in range(total_pages):
            page = doc[page_num]
            pix = page.get_pixmap(dpi=dpi)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG', quality=quality)
            base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
            images.append(base64_image)

        doc.close()
        logger.info(f"‚úÖ [IMG] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(images)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return images

    return await asyncio.to_thread(_extract)


async def extract_page_metadata(doc_content: bytes, filename: str, max_pages: int = None) -> List[Dict[str, Any]]:
    """
    Stage 1: –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü (—à—Ç–∞–º–ø, –∑–∞–≥–æ–ª–æ–≤–∫–∏) —á–µ—Ä–µ–∑ Vision API.
    –§–æ–∫—É—Å –Ω–∞ –ø—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–æ–ª (—à—Ç–∞–º–ø), –ø—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π, –∑–∞–≥–æ–ª–æ–≤–∫–∏.
    """
    if max_pages is None:
        max_pages = STAGE1_MAX_PAGES

    logger.info(f"üìã [STAGE 1] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ {filename}...")

    def _extract_crops():
        import base64
        from PIL import Image
        import io

        doc = fitz.open(stream=doc_content, filetype="pdf")
        metadata_images = []

        total_pages = min(len(doc), max_pages)

        for page_num in range(total_pages):
            page = doc[page_num]
            pix = page.get_pixmap(dpi=STAGE1_DPI)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            # –í—ã—Ä–µ–∑–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            width, height = img.size

            # –ü—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–æ–ª (—à—Ç–∞–º–ø)
            stamp_crop = img.crop((
                int(width * STAGE1_STAMP_CROP['left']),
                int(height * STAGE1_STAMP_CROP['top']),
                int(width * STAGE1_STAMP_CROP['right']),
                int(height * STAGE1_STAMP_CROP['bottom'])
            ))

            # –ü—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª
            top_right_crop = img.crop((
                int(width * STAGE1_TOP_RIGHT_CROP['left']),
                int(height * STAGE1_TOP_RIGHT_CROP['top']),
                int(width * STAGE1_TOP_RIGHT_CROP['right']),
                int(height * STAGE1_TOP_RIGHT_CROP['bottom'])
            ))

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–≤–µ—Ä—Ö–Ω—è—è —á–∞—Å—Ç—å)
            header_crop = img.crop((
                int(width * STAGE1_HEADER_CROP['left']),
                int(height * STAGE1_HEADER_CROP['top']),
                int(width * STAGE1_HEADER_CROP['right']),
                int(height * STAGE1_HEADER_CROP['bottom'])
            ))

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
            combined = Image.new('RGB', (width, int(height * 0.45)))
            combined.paste(header_crop, (0, 0))
            combined.paste(top_right_crop, (int(width * 0.7), int(height * 0.1)))
            combined.paste(stamp_crop, (int(width * 0.7), int(height * 0.25)))

            img_byte_arr = io.BytesIO()
            combined.save(img_byte_arr, format='JPEG', quality=STAGE1_QUALITY)
            base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
            metadata_images.append({
                'page_number': page_num + 1,
                'image': base64_image
            })

        doc.close()
        logger.info(f"‚úÖ [STAGE 1] –ò–∑–≤–ª–µ—á–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å {len(metadata_images)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return metadata_images

    crops = await asyncio.to_thread(_extract_crops)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü (–∑–∞—â–∏—Ç–∞ –æ—Ç 429 rate limit)
    if len(crops) > STAGE1_MAX_PAGES_PER_REQUEST:
        logger.warning(f"‚ö†Ô∏è [STAGE 1] –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü ({len(crops)} > {STAGE1_MAX_PAGES_PER_REQUEST})")
        logger.warning(f"‚ö†Ô∏è [STAGE 1] –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ {STAGE1_MAX_PAGES_PER_REQUEST} —Å—Ç—Ä–∞–Ω–∏—Ü...")

        all_pages_metadata = []
        for batch_start in range(0, len(crops), STAGE1_MAX_PAGES_PER_REQUEST):
            batch_end = min(batch_start + STAGE1_MAX_PAGES_PER_REQUEST, len(crops))
            batch_crops = crops[batch_start:batch_end]

            logger.info(f"üìÑ [STAGE 1] –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Å—Ç—Ä–∞–Ω–∏—Ü {batch_start+1}-{batch_end}...")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –±–∞—Ç—á–∞
            content = [{
                "type": "text",
                "text": """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö —á–µ—Ä—Ç–µ–∂–µ–π.
–î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑–≤–ª–µ–∫–∏:
- –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞/—Ä–∞–∑–¥–µ–ª–∞ (–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ —à—Ç–∞–º–ø–∞)
- –†–∞–∑–¥–µ–ª –ø—Ä–æ–µ–∫—Ç–∞ (–ê–†, –ö–†, –ò–°, –û–í, –í–ö, –≠–° –∏ —Ç.–¥.)
- –¢–∏–ø —á–µ—Ä—Ç–µ–∂–∞ (–ø–ª–∞–Ω, —Ä–∞–∑—Ä–µ–∑, —Å—Ö–µ–º–∞, —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è)
- –ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞

–í–ê–ñ–ù–û: –û–±—Ä–∞—Ç–∏ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞:
- –ü—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–æ–ª (—à—Ç–∞–º–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏)
- –ü—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–∏—Å—Ç–æ–≤

–í–µ—Ä–Ω–∏ JSON:
{
  "pages": [
    {"page": 1, "title": "–ü–ª–∞–Ω 1 —ç—Ç–∞–∂–∞", "section": "–ê–†", "type": "–ø–ª–∞–Ω", "sheet_number": "–ê–†-01"},
    {"page": 2, "title": "–°—Ö–µ–º–∞ —ç–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏—è", "section": "–≠–°", "type": "—Å—Ö–µ–º–∞", "sheet_number": "–≠–°-03"}
  ]
}"""
            }]

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–∞—Ç—á–∞
            for item in batch_crops:
                content.append({
                    "type": "text",
                    "text": f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {item['page_number']} ---"
                })
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{item['image']}",
                        "detail": "low"
                    }
                })

            try:
                response = await client.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=[{"role": "user", "content": content}],
                    temperature=TEMPERATURE,
                    response_format={"type": "json_object"},
                    max_tokens=4000
                )

                data = json.loads(response.choices[0].message.content)
                batch_metadata = data.get('pages', [])
                all_pages_metadata.extend(batch_metadata)
                logger.info(f"‚úÖ [STAGE 1] –ë–∞—Ç—á {batch_start+1}-{batch_end}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(batch_metadata)} –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")

            except Exception as e:
                logger.error(f"‚ùå [STAGE 1] –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_start+1}-{batch_end}: {e}")
                # Fallback –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
                for item in batch_crops:
                    all_pages_metadata.append({
                        "page": item['page_number'],
                        "title": f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {item['page_number']}",
                        "section": "Unknown",
                        "type": "unknown",
                        "sheet_number": f"{item['page_number']}"
                    })

        logger.info(f"‚úÖ [STAGE 1] –ò–∑–≤–ª–µ—á–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(all_pages_metadata)} —Å—Ç—Ä–∞–Ω–∏—Ü (–≤—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {(len(crops)-1)//STAGE1_MAX_PAGES_PER_REQUEST + 1})")
        return all_pages_metadata

    # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –º–∞–ª–æ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    logger.info(f"üîç [STAGE 1] –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Vision API...")

    content = [{
        "type": "text",
        "text": """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö —á–µ—Ä—Ç–µ–∂–µ–π.
–î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑–≤–ª–µ–∫–∏:
- –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞/—Ä–∞–∑–¥–µ–ª–∞ (–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ —à—Ç–∞–º–ø–∞)
- –†–∞–∑–¥–µ–ª –ø—Ä–æ–µ–∫—Ç–∞ (–ê–†, –ö–†, –ò–°, –û–í, –í–ö, –≠–° –∏ —Ç.–¥.)
- –¢–∏–ø —á–µ—Ä—Ç–µ–∂–∞ (–ø–ª–∞–Ω, —Ä–∞–∑—Ä–µ–∑, —Å—Ö–µ–º–∞, —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è)
- –ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞

–í–ê–ñ–ù–û: –û–±—Ä–∞—Ç–∏ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞:
- –ü—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–æ–ª (—à—Ç–∞–º–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏)
- –ü—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–∏—Å—Ç–æ–≤

–í–µ—Ä–Ω–∏ JSON:
{
  "pages": [
    {"page": 1, "title": "–ü–ª–∞–Ω 1 —ç—Ç–∞–∂–∞", "section": "–ê–†", "type": "–ø–ª–∞–Ω", "sheet_number": "–ê–†-01"},
    {"page": 2, "title": "–°—Ö–µ–º–∞ —ç–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏—è", "section": "–≠–°", "type": "—Å—Ö–µ–º–∞", "sheet_number": "–≠–°-03"}
  ]
}"""
    }]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    for item in crops:
        content.append({
            "type": "text",
            "text": f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {item['page_number']} ---"
        })
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{item['image']}",
                "detail": "low"
            }
        })

    try:
        response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": content}],
            temperature=TEMPERATURE,
            response_format={"type": "json_object"},
            max_tokens=4000
        )

        data = json.loads(response.choices[0].message.content)
        pages_metadata = data.get('pages', [])
        logger.info(f"‚úÖ [STAGE 1] –ò–∑–≤–ª–µ—á–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(pages_metadata)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return pages_metadata

    except Exception as e:
        logger.error(f"‚ùå [STAGE 1] –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        return [{"page": i+1, "title": f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}", "section": "Unknown", "type": "unknown", "sheet_number": f"{i+1}"}
                for i in range(len(crops))]


async def assess_page_relevance(
    pages_metadata: List[Dict[str, Any]],
    doc_images_low: List[str],
    requirements: List[Dict[str, Any]]
) -> Dict[int, List[int]]:
    """
    Stage 2: –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç mapping: {requirement_number: [page_numbers]}

    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è gpt-4o-mini: –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º Vision API —Å high-res
    """
    logger.info(f"üîç [STAGE 2] –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ {len(pages_metadata)} —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è {len(requirements)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π...")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Ä–∞–∑–±–∏–≤–∞—Ç—å –Ω–∞ –±–∞—Ç—á–∏
    if len(doc_images_low) > STAGE2_MAX_PAGES_PER_REQUEST:
        logger.warning(f"‚ö†Ô∏è [STAGE 2] –ú–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü ({len(doc_images_low)} > {STAGE2_MAX_PAGES_PER_REQUEST})")
        logger.warning(f"‚ö†Ô∏è [STAGE 2] –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ {STAGE2_MAX_PAGES_PER_REQUEST} —Å—Ç—Ä–∞–Ω–∏—Ü...")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
        all_page_mappings = []
        for batch_start in range(0, len(doc_images_low), STAGE2_MAX_PAGES_PER_REQUEST):
            batch_end = min(batch_start + STAGE2_MAX_PAGES_PER_REQUEST, len(doc_images_low))
            batch_images = doc_images_low[batch_start:batch_end]
            batch_metadata = pages_metadata[batch_start:batch_end]

            logger.info(f"üìÑ [STAGE 2] –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Å—Ç—Ä–∞–Ω–∏—Ü {batch_start+1}-{batch_end}...")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞—Ç—á
            batch_mapping = await _analyze_relevance_batch(batch_metadata, batch_images, requirements, batch_start)
            all_page_mappings.append(batch_mapping)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –±–∞—Ç—á–µ–π
        combined_mapping = {}
        for req in requirements:
            req_num = req['number']
            combined_pages = []
            for batch_mapping in all_page_mappings:
                if req_num in batch_mapping:
                    combined_pages.extend(batch_mapping[req_num])
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            combined_mapping[req_num] = sorted(list(set(combined_pages)))
            logger.info(f"üìÑ [STAGE 2] Req {req_num}: —Å—Ç—Ä–∞–Ω–∏—Ü—ã {combined_mapping[req_num][:5]}{'...' if len(combined_mapping[req_num]) > 5 else ''}")

        logger.info(f"‚úÖ [STAGE 2] –ü–æ—Å—Ç—Ä–æ–µ–Ω mapping –¥–ª—è {len(combined_mapping)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π")
        return combined_mapping

    # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ–º–Ω–æ–≥–æ - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
    return await _analyze_relevance_batch(pages_metadata, doc_images_low, requirements, 0)


async def _analyze_relevance_batch(
    batch_metadata: List[Dict[str, Any]],
    batch_images: List[str],
    requirements: List[Dict[str, Any]],
    offset: int = 0
) -> Dict[int, List[int]]:
    """
    –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞—Ç—á–∞ —Å—Ç—Ä–∞–Ω–∏—Ü.
    offset - —Å–º–µ—â–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–∏
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü
    pages_description = "\n".join([
        f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {p['page']}: {p.get('title', 'N/A')} [{p.get('section', 'N/A')}] - {p.get('type', 'N/A')}"
        for p in batch_metadata
    ])

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    requirements_text = "\n".join([
        f"{req['number']}. [{req.get('section', '–û–±—â–∏–µ')}] {req['text'][:200]}..."
        for req in requirements
    ])

    content = [{
        "type": "text",
        "text": f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

–ü–µ—Ä–µ–¥ —Ç–æ–±–æ–π {len(batch_images)} —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –í–´–°–û–ö–û–ú —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ (gpt-4o-mini –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è).
–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –¢–û–ß–ù–û –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.

–ú–ï–¢–ê–î–ê–ù–ù–´–ï –°–¢–†–ê–ù–ò–¶:
{pages_description}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ò–ó –¢–ó:
{requirements_text}

–í–ê–ñ–ù–û –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
- –ü—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–æ–ª (—à—Ç–∞–º–ø) - –Ω–æ–º–µ—Ä–∞ –ª–∏—Å—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—ã
- –ü—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ - –Ω–∞–∑–≤–∞–Ω–∏—è –ø–ª–∞–Ω–æ–≤, —Å—Ö–µ–º, —Ä–∞–∑—Ä–µ–∑–æ–≤

–ü–†–ò–ù–¶–ò–ü–´ –û–¢–ë–û–†–ê –°–¢–†–ê–ù–ò–¶:
1. –í–∫–ª—é—á–∞–π –¢–û–õ–¨–ö–û —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ü–†–Ø–ú–û–ô —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
2. –û–ø—Ç–∏–º–∞–ª—å–Ω–æ: 3-7 —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ (–Ω–µ –±–æ–ª–µ–µ 10)
3. –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–ø–ª–∞–Ω—ã, —Å—Ö–µ–º—ã, —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏) –Ω–∞–¥ –æ–±—â–∏–º–∏ –ª–∏—Å—Ç–∞–º–∏
4. –ò—Å–∫–ª—é—á–∞–π –¥—É–±–ª–∏ –∏ —Å–º–µ–∂–Ω—ã–µ –ª–∏—Å—Ç—ã —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–π—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
5. –ï—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ - –≤—ã–±–∏—Ä–∞–π –∫–ª—é—á–µ–≤—ã–µ –ª–∏—Å—Ç—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞

–ü–†–ò–ú–ï–†–´:
- "–í—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤ 2.64–º" ‚Üí —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–ª–∞–Ω–∞–º–∏ —ç—Ç–∞–∂–µ–π –∏ —Ä–∞–∑—Ä–µ–∑–∞–º–∏ (3-5 –ª–∏—Å—Ç–æ–≤ –ê–†)
- "–ü–∞—Ä–∫–∏–Ω–≥ 2-—É—Ä–æ–≤–Ω–µ–≤—ã–π" ‚Üí –≥–µ–Ω–ø–ª–∞–Ω + –ø–ª–∞–Ω—ã –ø–∞—Ä–∫–∏–Ω–≥–∞ (2-4 –ª–∏—Å—Ç–∞)
- "–õ–∏—Ñ—Ç—ã –≥—Ä—É–∑–æ–ø–∞—Å—Å–∞–∂–∏—Ä—Å–∫–∏–µ" ‚Üí –ø–ª–∞–Ω—ã —Å –ª–∏—Ñ—Ç–æ–≤—ã–º–∏ —à–∞—Ö—Ç–∞–º–∏ + —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ñ—Ç–æ–≤ (4-6 –ª–∏—Å—Ç–æ–≤)

–í–µ—Ä–Ω–∏ JSON:
{{
  "page_mapping": [
    {{"requirement_number": 1, "relevant_pages": [3, 15, 22], "reason": "–ü–ª–∞–Ω 1 —ç—Ç–∞–∂–∞, —Ä–∞–∑—Ä–µ–∑ 1-1, —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—ã—Å–æ—Ç"}},
    {{"requirement_number": 2, "relevant_pages": [45, 46], "reason": "–°—Ö–µ–º–∞ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏–∏ –û–í, —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è"}}
  ]
}}

–í–ê–ñ–ù–û: –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ —ç–∫–æ–Ω–æ–º–Ω—ã–º. –õ—É—á—à–µ 5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, —á–µ–º 25 –≤–æ–∑–º–æ–∂–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö."""
    }]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –í–´–°–û–ö–û–ú –∫–∞—á–µ—Å—Ç–≤–µ (gpt-4o-mini –¥–µ—à–µ–≤–∞—è, –Ω–µ —ç–∫–æ–Ω–æ–º–∏–º)
    for idx, base64_image in enumerate(batch_images, 1):
        page_num = offset + idx
        content.append({
            "type": "text",
            "text": f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ---"
        })
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_image}",
                "detail": STAGE2_DETAIL  # "high" –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            }
        })

    try:
        response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": content}],
            temperature=TEMPERATURE,
            response_format={"type": "json_object"},
            max_tokens=4000
        )

        data = json.loads(response.choices[0].message.content)
        page_mapping_list = data.get('page_mapping', [])

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å {requirement_number: [pages]}
        page_mapping = {}
        for item in page_mapping_list:
            req_num = item.get('requirement_number')
            pages = item.get('relevant_pages', [])
            reason = item.get('reason', '')
            page_mapping[req_num] = pages
            logger.info(f"üìÑ [STAGE 2] Req {req_num}: —Å—Ç—Ä–∞–Ω–∏—Ü—ã {pages} ({reason})")

        logger.info(f"‚úÖ [STAGE 2] –ü–æ—Å—Ç—Ä–æ–µ–Ω mapping –¥–ª—è {len(page_mapping)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π")
        return page_mapping

    except Exception as e:
        logger.error(f"‚ùå [STAGE 2] –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –±–∞—Ç—á–∞: {e}")
        # Fallback: –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞ –¥–ª—è –≤—Å–µ—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        logger.warning(f"‚ö†Ô∏è [STAGE 2] –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è –±–∞—Ç—á–∞ - —Å—Ç—Ä–∞–Ω–∏—Ü—ã {offset+1}-{offset+len(batch_images)}")
        return {req['number']: list(range(offset + 1, offset + len(batch_images) + 1)) for req in requirements}


@retry(stop=stop_after_attempt(RETRY_MAX_ATTEMPTS), wait=wait_exponential(multiplier=RETRY_WAIT_EXPONENTIAL_MULTIPLIER, min=4, max=RETRY_WAIT_EXPONENTIAL_MAX))
async def find_contradictions(
    pages_metadata: List[Dict[str, Any]],
    doc_content: bytes,
    requirements: List[Dict[str, Any]],
    analyzed_reqs: List['RequirementAnalysis']
) -> str:
    """
    Stage 4: –ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –∏—â–µ—Ç –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è.

    Returns: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è—Ö
    """
    logger.info(f"üîç [STAGE 4] –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
    sections = {}
    for page_meta in pages_metadata:
        section = page_meta.get('section', 'N/A')
        if section not in sections:
            sections[section] = []
        sections[section].append(page_meta)

    logger.info(f"üìä [STAGE 4] –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {list(sections.keys())}")

    # –û—Ç–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
    selected_pages = []
    for section, pages in sections.items():
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ N —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
        sample = pages[:STAGE4_SAMPLE_PAGES_PER_SECTION]
        selected_pages.extend([p['page'] for p in sample])
        logger.info(f"üìÑ [STAGE 4] –†–∞–∑–¥–µ–ª {section}: –≤—ã–±—Ä–∞–Ω–æ {len(sample)} —Å—Ç—Ä–∞–Ω–∏—Ü")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å—Ä–µ–¥–Ω–µ–º –∫–∞—á–µ—Å—Ç–≤–µ
    logger.info(f"üìÑ [STAGE 4] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ {len(selected_pages)} –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")

    def _extract_pages():
        import base64
        from PIL import Image
        import io

        doc = fitz.open(stream=doc_content, filetype="pdf")
        images = []

        for page_num in selected_pages:
            if page_num < 1 or page_num > len(doc):
                continue
            page = doc[page_num - 1]
            pix = page.get_pixmap(dpi=STAGE4_DPI)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG', quality=STAGE4_QUALITY)
            base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
            images.append({'page': page_num, 'image': base64_image})

        doc.close()
        logger.info(f"‚úÖ [STAGE 4] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(images)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return images

    doc_images = await asyncio.to_thread(_extract_pages)

    # –§–æ—Ä–º–∏—Ä—É–µ–º summary –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    requirements_summary = "\n".join([
        f"{r.number}. {r.requirement[:100]}... ‚Üí {r.status} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {r.confidence}%)"
        for r in analyzed_reqs[:20]  # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    ])

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
    content = [{
        "type": "text",
        "text": f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ–µ–∫—Ç–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ü–†–û–¢–ò–í–û–†–ï–ß–ò–ô –∏ –ù–ï–°–û–û–¢–í–ï–¢–°–¢–í–ò–ô.

–ö–û–ù–¢–ï–ö–°–¢ –¢–†–ï–ë–û–í–ê–ù–ò–ô –ò–ó –¢–ó (—É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã):
{requirements_summary}

–†–ê–ó–î–ï–õ–´ –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò:
{', '.join(sections.keys())}

–ó–ê–î–ê–ß–ê:
–ù–∞–π–¥–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É:
1. –†–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞ (–ê–† vs –ö–†, –ò–° vs –û–í –∏ —Ç.–¥.)
2. –ü–ª–∞–Ω–∞–º–∏ –∏ —Ä–∞–∑—Ä–µ–∑–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
3. –¢–µ–∫—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏
4. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –¢–ó –∏ –ø—Ä–æ–µ–∫—Ç–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏

–¢–ò–ü–´ –ü–†–û–¢–ò–í–û–†–ï–ß–ò–ô:
- –†–∞–∑–º–µ—Ä–Ω—ã–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (–≤—ã—Å–æ—Ç—ã, –ø–ª–æ—â–∞–¥–∏, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è)
- –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–ª–∏—Ñ—Ç—ã, –ø–∞—Ä–∫–æ–≤–∫–∏, –ø–æ–º–µ—â–µ–Ω–∏—è)
- –ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ –∏ –Ω—É–º–µ—Ä–∞—Ü–∏–∏
- –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö
- –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö

–í–ê–ñ–ù–û:
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ö–û–ù–ö–†–ï–¢–ù–´–ï –¥–∞–Ω–Ω—ã–µ —Å –ª–∏—Å—Ç–æ–≤ (–Ω–æ–º–µ—Ä–∞, —Ä–∞–∑–º–µ—Ä—ã, –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏)
- –£–∫–∞–∑—ã–≤–∞–π –¢–û–ß–ù–´–ï —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã/–ª–∏—Å—Ç—ã
- –û—Ü–µ–Ω–∏–≤–∞–π –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è (–∫—Ä–∏—Ç–∏—á–Ω–æ/—Å—Ä–µ–¥–Ω–µ/–Ω–∏–∑–∫–æ)
- –ï—Å–ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –Ω–µ—Ç - —Ç–∞–∫ –∏ —É–∫–∞–∂–∏

–í–µ—Ä–Ω–∏ JSON:
{{
  "contradictions": [
    {{
      "type": "–†–∞–∑–º–µ—Ä–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ",
      "severity": "–∫—Ä–∏—Ç–∏—á–Ω–æ",
      "description": "–í—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤: –ê–† —É–∫–∞–∑—ã–≤–∞–µ—Ç 2.70–º (–ª–∏—Å—Ç –ê–†-03), –ö–† –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 2.64–º (–ª–∏—Å—Ç –ö–†-02)",
      "pages": [3, 15],
      "recommendation": "–£—Ç–æ—á–Ω–∏—Ç—å –ø—Ä–æ–µ–∫—Ç–Ω—É—é –≤—ã—Å–æ—Ç—É —Å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏"
    }}
  ],
  "summary": "–ù–∞–π–¥–µ–Ω–æ N –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π: X –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö, Y —Å—Ä–µ–¥–Ω–∏—Ö, Z –Ω–∏–∑–∫–∏—Ö. –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –ê–† –∏ –ö–†."
}}

–ü–µ—Ä–µ–¥ —Ç–æ–±–æ–π {len(doc_images)} –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""
    }]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    for img_data in doc_images:
        content.append({
            "type": "text",
            "text": f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {img_data['page']} ---"
        })
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{img_data['image']}",
                "detail": STAGE4_DETAIL
            }
        })

    try:
        response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": content}],
            temperature=TEMPERATURE,
            response_format={"type": "json_object"},
            max_tokens=STAGE4_MAX_TOKENS
        )

        result = json.loads(response.choices[0].message.content)
        contradictions = result.get('contradictions', [])
        summary = result.get('summary', '–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω')

        logger.info(f"‚úÖ [STAGE 4] –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π: {len(contradictions)}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        if not contradictions:
            return "‚úÖ –ü–†–û–¢–ò–í–û–†–ï–ß–ò–ô –ù–ï –û–ë–ù–ê–†–£–ñ–ï–ù–û\n\n–ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —è–≤–Ω—ã—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏."

        report = f"üîç –û–¢–ß–ï–¢ –û –ü–†–û–¢–ò–í–û–†–ï–ß–ò–Ø–•\n\n{summary}\n\n"
        report += "=" * 80 + "\n\n"

        for idx, contr in enumerate(contradictions, 1):
            severity_emoji = {"–∫—Ä–∏—Ç–∏—á–Ω–æ": "üî¥", "—Å—Ä–µ–¥–Ω–µ": "üü°", "–Ω–∏–∑–∫–æ": "üü¢"}.get(contr.get('severity', '—Å—Ä–µ–¥–Ω–µ'), "‚ö™")
            report += f"{idx}. {severity_emoji} {contr.get('type', '–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ').upper()}\n"
            report += f"   –ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å: {contr.get('severity', '—Å—Ä–µ–¥–Ω–µ')}\n"
            report += f"   –û–ø–∏—Å–∞–Ω–∏–µ: {contr.get('description', 'N/A')}\n"
            report += f"   –°—Ç—Ä–∞–Ω–∏—Ü—ã: {', '.join(map(str, contr.get('pages', [])))}\n"
            report += f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {contr.get('recommendation', '–¢—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ')}\n\n"

        return report

    except Exception as e:
        logger.error(f"‚ùå [STAGE 4] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π: {e}")
        return f"‚ö†Ô∏è –û–®–ò–ë–ö–ê –ê–ù–ê–õ–ò–ó–ê –ü–†–û–¢–ò–í–û–†–ï–ß–ò–ô\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑: {str(e)}"


def get_analysis_system_prompt(stage: str, req_type: str) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç system prompt –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
    """
    stage_prompt = PROMPTS.get(stage, PROMPTS["–§–≠"])

    return f"""{stage_prompt}

–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞:

1. –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ {req_type}
2. –ù–∞–π—Ç–∏ –≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (—á–µ—Ä—Ç–µ–∂–∞—Ö) —Ä–µ—à–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
3. –í–µ—Ä–Ω—É—Ç—å –∞–Ω–∞–ª–∏–∑ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:

{{
  "number": <–Ω–æ–º–µ—Ä —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è>,
  "requirement": "<—Ç–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è>",
  "status": "<–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è>",
  "confidence": <0-100>,
  "solution_description": "<–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ>",
  "reference": "<–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞: –Ω–æ–º–µ—Ä –ª–∏—Å—Ç–∞, —Ä–∞–∑–¥–µ–ª, —Å—Ç—Ä–∞–Ω–∏—Ü–∞>",
  "discrepancies": "<–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ '-'>",
  "recommendations": "<—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–ª–∏ '-'>"
}}

–í–ê–ñ–ù–û:
- –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ –≤—Å–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä—Ç–µ–∂–µ–π
- –£–∫–∞–∑—ã–≤–∞–π –ö–û–ù–ö–†–ï–¢–ù–´–ï —Å—Å—ã–ª–∫–∏ (–Ω–æ–º–µ—Ä–∞ –ª–∏—Å—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—ã, —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∫ —Ç–µ–∫—Å—Ç, —Ç–∞–∫ –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂–∞—Ö
- –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —É–∫–∞–∑—ã–≤–∞–π status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è"
- –í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π
"""


async def analyze_batch_with_high_detail(
    system_prompt: str,
    doc_content: bytes,
    page_numbers: List[int],
    requirements_batch: List[Dict[str, Any]],
    request: Request
) -> List['RequirementAnalysis']:
    """
    Stage 3: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞–∫–µ—Ç–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å –í–´–°–û–ö–ò–ú —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    if await request.is_disconnected():
        logger.warning(f"‚ö†Ô∏è [STAGE 3] Client disconnected")
        return []

    batch_ids = [req['trace_id'] for req in requirements_batch]
    logger.info(f"üîç [STAGE 3] –ê–Ω–∞–ª–∏–∑ {len(requirements_batch)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö {page_numbers[:5]}{'...' if len(page_numbers) > 5 else ''}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü (–∑–∞—â–∏—Ç–∞ –æ—Ç 429 rate limit)
    if len(page_numbers) > STAGE3_MAX_PAGES_PER_REQUEST:
        logger.warning(f"‚ö†Ô∏è [STAGE 3] –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü ({len(page_numbers)} > {STAGE3_MAX_PAGES_PER_REQUEST})")
        logger.warning(f"‚ö†Ô∏è [STAGE 3] –†–∞–∑–±–∏–≤–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º...")

        # –†–∞–∑–±–∏–≤–∞–µ–º page_numbers –Ω–∞ —á–∞–Ω–∫–∏
        all_results = []
        for i in range(0, len(page_numbers), STAGE3_MAX_PAGES_PER_REQUEST):
            chunk_pages = page_numbers[i:i + STAGE3_MAX_PAGES_PER_REQUEST]
            logger.info(f"üìÑ [STAGE 3] –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã —Å—Ç—Ä–∞–Ω–∏—Ü {i+1}-{min(i+STAGE3_MAX_PAGES_PER_REQUEST, len(page_numbers))}")

            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü
            chunk_results = await analyze_batch_with_high_detail(
                system_prompt=system_prompt,
                doc_content=doc_content,
                page_numbers=chunk_pages,
                requirements_batch=requirements_batch,
                request=request
            )

            if not chunk_results:
                return []  # Client disconnected

            all_results.extend(chunk_results)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–æ–º–µ—Ä—É —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        seen = set()
        unique_results = []
        for result in all_results:
            if result.number not in seen:
                seen.add(result.number)
                unique_results.append(result)

        return unique_results

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ
    logger.info(f"üìÑ [STAGE 3] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ {len(page_numbers)} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ...")

    def _extract_pages():
        import base64
        from PIL import Image
        import io

        doc = fitz.open(stream=doc_content, filetype="pdf")
        images = []

        for page_num in page_numbers:
            if page_num < 1 or page_num > len(doc):
                continue
            page = doc[page_num - 1]  # page_num –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 1
            pix = page.get_pixmap(dpi=STAGE3_DPI)  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG', quality=STAGE3_QUALITY)  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
            images.append(base64_image)

        doc.close()
        logger.info(f"‚úÖ [STAGE 3] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(images)} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ")
        return images

    doc_images_high = await asyncio.to_thread(_extract_pages)

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    requirements_text = "\n\n".join([
        f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ {req['number']} [{req.get('section', '–û–±—â–∏–µ')}]:\n{req['text']}"
        for req in requirements_batch
    ])

    content = [{
        "type": "text",
        "text": f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ –¢–ó –∏ –Ω–∞–π–¥–∏ –¥–ª—è –ö–ê–ñ–î–û–ì–û —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

{requirements_text}

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "analyses": [
    {{
      "number": <–Ω–æ–º–µ—Ä —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è>,
      "requirement": "<—Ç–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è>",
      "status": "<–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ|–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è>",
      "confidence": <0-100>,
      "solution_description": "<–æ–ø–∏—Å–∞–Ω–∏–µ>",
      "reference": "<–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞: –Ω–æ–º–µ—Ä –ª–∏—Å—Ç–∞, —Ä–∞–∑–¥–µ–ª, —Å—Ç—Ä–∞–Ω–∏—Ü–∞>",
      "discrepancies": "<–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ '-'>",
      "recommendations": "<—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–ª–∏ '-'>"
    }}
  ]
}}

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –∞–Ω–∞–ª–∏–∑ –¥–ª—è –í–°–ï–• {len(requirements_batch)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ!
–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –í–´–°–û–ö–û–ú —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ - –∏–∑—É—á–∞–π –¥–µ—Ç–∞–ª–∏, —Ç–µ–∫—Å—Ç, —Ä–∞–∑–º–µ—Ä—ã, –º–∞—Ä–∫–∏—Ä–æ–≤–∫—É."""
    }]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ
    for idx, base64_image in enumerate(doc_images_high, 1):
        page_num = page_numbers[idx - 1] if idx <= len(page_numbers) else idx
        content.append({
            "type": "text",
            "text": f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ---"
        })
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_image}",
                "detail": STAGE3_DETAIL  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            }
        })

    try:
        response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content}
            ],
            temperature=TEMPERATURE,
            response_format={"type": "json_object"},  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π JSON
            max_tokens=STAGE3_MAX_TOKENS
        )

        response_text = response.choices[0].message.content
        refusal = response.choices[0].message.refusal

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None –∏–ª–∏ refusal
        if response_text is None or refusal:
            logger.error(f"‚ùå [STAGE 3] Model refused to respond!")
            logger.error(f"Refusal message: {refusal}")
            logger.error(f"Finish reason: {response.choices[0].finish_reason}")
            logger.error(f"Requirements in batch: {[req['number'] for req in requirements_batch]}")
            logger.error(f"Requirements text preview: {[req['text'][:100] for req in requirements_batch]}")

            # –ï—Å–ª–∏ –±–∞—Ç—á –±–æ–ª—å—à–µ 1 –∏ –≤–∫–ª—é—á–µ–Ω retry - –ø—Ä–æ–±—É–µ–º –ø–æ –æ–¥–Ω–æ–º—É
            if STAGE3_RETRY_ON_REFUSAL and len(requirements_batch) > 1:
                logger.warning(f"‚ö†Ô∏è [STAGE 3] Retry: analyzing {len(requirements_batch)} requirements individually...")
                all_results = []
                for single_req in requirements_batch:
                    try:
                        single_result = await analyze_batch_with_high_detail(
                            system_prompt=system_prompt,
                            doc_content=doc_content,
                            page_numbers=page_numbers,
                            requirements_batch=[single_req],
                            request=request
                        )
                        all_results.extend(single_result)
                    except Exception as e:
                        logger.error(f"‚ùå [STAGE 3] Failed to analyze requirement {single_req['number']}: {e}")
                        all_results.append(RequirementAnalysis(
                            number=single_req['number'],
                            requirement=single_req['text'],
                            status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
                            confidence=0,
                            solution_description="–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ",
                            reference="-",
                            discrepancies=str(e),
                            recommendations="–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é",
                            section=single_req.get('section'),
                            trace_id=single_req['trace_id']
                        ))
                return all_results

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            return [
                RequirementAnalysis(
                    number=req['number'],
                    requirement=req['text'],
                    status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
                    confidence=0,
                    solution_description="–ú–æ–¥–µ–ª—å –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
                    reference="-",
                    discrepancies=f"Content filter: {refusal or 'Response is None'}",
                    recommendations="–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é",
                    section=req.get('section'),
                    trace_id=req['trace_id']
                )
                for req in requirements_batch
            ]

        logger.info(f"üìÑ [STAGE 3] Response preview: {response_text[:LOG_RESPONSE_PREVIEW_LENGTH]}...")

        # –ü–∞—Ä—Å–∏–º JSON
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            data = json.loads(json_str)

            analyses = data.get('analyses', [])
            req_map = {req['number']: req for req in requirements_batch}

            results = []
            for analysis in analyses:
                req_num = analysis.get('number')
                if req_num in req_map:
                    req = req_map[req_num]
                    results.append(RequirementAnalysis(
                        **analysis,
                        section=req.get('section'),
                        trace_id=req['trace_id']
                    ))

            logger.info(f"‚úÖ [STAGE 3] –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results)}/{len(requirements_batch)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π")

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö
            if len(results) < len(requirements_batch):
                analyzed_numbers = {r.number for r in results}
                missing = [req['number'] for req in requirements_batch if req['number'] not in analyzed_numbers]
                logger.warning(f"‚ö†Ô∏è [STAGE 3] –ü—Ä–æ–ø—É—â–µ–Ω—ã —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: {missing}")
                for req in requirements_batch:
                    if req['number'] not in analyzed_numbers:
                        results.append(RequirementAnalysis(
                            number=req['number'],
                            requirement=req['text'],
                            status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
                            confidence=0,
                            solution_description="–ù–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ",
                            reference="-",
                            discrepancies="–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏",
                            recommendations="–ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∞–Ω–∞–ª–∏–∑",
                            section=req.get('section'),
                            trace_id=req['trace_id']
                        ))

            results.sort(key=lambda r: r.number)
            return results
        else:
            logger.error(f"‚ùå [STAGE 3] No JSON in response. Full response: {response_text[:500]}")
            raise ValueError("No JSON found in response")

    except RateLimitError as e:
        error_msg = str(e)
        logger.error(f"‚ùå [STAGE 3] Rate limit exceeded: {error_msg}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ TPM –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        if "tokens per min" in error_msg.lower() and len(page_numbers) > 10:
            logger.warning(f"‚ö†Ô∏è [STAGE 3] –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑-–∑–∞ {len(page_numbers)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            logger.warning(f"‚ö†Ô∏è [STAGE 3] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏...")

            # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–ø–æ–ª–∞–º
            mid = len(page_numbers) // 2
            chunk1_pages = page_numbers[:mid]
            chunk2_pages = page_numbers[mid:]

            all_results = []
            for chunk_pages in [chunk1_pages, chunk2_pages]:
                chunk_results = await analyze_batch_with_high_detail(
                    system_prompt=system_prompt,
                    doc_content=doc_content,
                    page_numbers=chunk_pages,
                    requirements_batch=requirements_batch,
                    request=request
                )
                if not chunk_results:
                    return []
                all_results.extend(chunk_results)

            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen = set()
            unique_results = []
            for result in all_results:
                if result.number not in seen:
                    seen.add(result.number)
                    unique_results.append(result)

            return unique_results

        # –û–±—ã—á–Ω–∞—è 429 –æ—à–∏–±–∫–∞ - retry –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç
        raise

    except Exception as e:
        logger.error(f"‚ùå [STAGE 3] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        if 'response_text' in locals() and response_text is not None:
            logger.error(f"Full response text: {response_text[:1000]}")
        else:
            logger.error("Response text is None or not available")
        return [
            RequirementAnalysis(
                number=req['number'],
                requirement=req['text'],
                status="–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
                confidence=0,
                solution_description="–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                reference="-",
                discrepancies=str(e),
                recommendations="–ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∞–Ω–∞–ª–∏–∑",
                section=req.get('section'),
                trace_id=req['trace_id']
            )
            for req in requirements_batch
        ]




# ============================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –¢–ó/–¢–£
# ============================

@retry(stop=stop_after_attempt(RETRY_MAX_ATTEMPTS), wait=wait_exponential(multiplier=RETRY_WAIT_EXPONENTIAL_MULTIPLIER, min=4, max=RETRY_WAIT_EXPONENTIAL_MAX))
async def extract_text_from_pdf(content: bytes, filename: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OCR –µ—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è."""
    import base64
    from PIL import Image
    import io

    text = ""
    doc = fitz.open(stream=content, filetype="pdf")
    is_scanned = True

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
    for page in doc:
        page_text = page.get_text()
        if page_text.strip():
            is_scanned = False
            text += page_text + "\n\n"

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR —á–µ—Ä–µ–∑ OpenAI Vision
    if is_scanned or not text.strip():
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {filename} –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω, –ø—Ä–∏–º–µ–Ω—è–µ–º OCR —á–µ—Ä–µ–∑ OpenAI Vision...")

        for page_num, page in enumerate(doc):
            # –†–µ–Ω–¥–µ—Ä–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            pix = page.get_pixmap(dpi=100)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG', quality=70)
            base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')

            # OCR —á–µ—Ä–µ–∑ Vision
            logger.info(f"üìÑ OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}/{len(doc)} –∏–∑ {filename}")
            response = await client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–æ–≤, —Ç–∞–±–ª–∏—Ü—ã. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                            }
                        ]
                    }
                ],
                temperature=0.0,
                max_tokens=4000
            )

            page_text = response.choices[0].message.content
            text += f"\n\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n\n{page_text}"

        logger.info(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {filename}, –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

    doc.close()

    if not text.strip():
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {filename}")
        return "[–î–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç]"

    return text.strip()


@retry(stop=stop_after_attempt(RETRY_MAX_ATTEMPTS), wait=wait_exponential(multiplier=RETRY_WAIT_EXPONENTIAL_MULTIPLIER, min=4, max=RETRY_WAIT_EXPONENTIAL_MAX))
async def segment_requirements(tz_text: str) -> List[Dict[str, Any]]:
    """–°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –¢–ó –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è GPT."""
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –¢–ó –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–∑ –Ω–µ–≥–æ —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.

–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∂–∏:
- number: –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ)
- text: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- section: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
- trace_id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'req-{{number}}'

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{"requirements": [{{"number": 1, "text": "...", "section": "...", "trace_id": "req-1"}}]}}

–¢–µ–∫—Å—Ç –¢–ó:
{tz_text[:10000]}"""  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10000 —Å–∏–º–≤–æ–ª–æ–≤

    response = await client.chat.completions.create(
        model="gpt-4o-mini",  # –î–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        messages=[{"role": "user", "content": prompt}],
        temperature=TEMPERATURE,
        response_format={"type": "json_object"}
    )

    try:
        data = json.loads(response.choices[0].message.content)
        requirements = data.get("requirements", [])
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(requirements)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π")
        return requirements
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Failed to parse requirements JSON: {e}")
        raise ValueError("Failed to parse requirements JSON")




async def _get_file_size(file: UploadFile) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö."""
    content = await file.read()
    await file.seek(0)
    return len(content)


# ============================
# PYDANTIC –ú–û–î–ï–õ–ò
# ============================

class RequirementAnalysis(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"""
    number: int
    requirement: str
    status: str
    confidence: int
    solution_description: str
    reference: str
    discrepancies: str
    recommendations: str
    section: Optional[str] = None
    trace_id: Optional[str] = None


class AnalysisResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    stage: str
    req_type: str
    requirements: List[RequirementAnalysis]
    summary: str


# ============================
# FASTAPI –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ============================

app = FastAPI(
    title="Document Analysis API (Vision Mode)",
    description="API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Vision API",
    version="6.0.0-vision"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================
# API ENDPOINTS
# ============================

@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "ok",
        "service": "Document Analysis API (VISION MODE)",
        "architecture": "TZ/TU manual parsing + Drawings via Vision API",
        "provider": "openai",
        "model": OPENAI_MODEL,
        "max_file_size_mb": MAX_FILE_SIZE_MB
    }


@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_documentation(
    request: Request,
    stage: str = Form(...),
    check_tu: bool = Form(False),
    req_type: str = Form("–¢–ó"),
    tz_document: UploadFile = File(...),
    doc_document: UploadFile = File(...),
    tu_document: Optional[UploadFile] = File(None)
):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
    - –¢–ó/–¢–£: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    - –ß–µ—Ä—Ç–µ–∂–∏: Vision API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–∫–ª—é—á–∏–ª—Å—è –ª–∏ –∫–ª–∏–µ–Ω—Ç
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected before analysis started. Aborting.")
            raise HTTPException(status_code=499, detail="Client disconnected")

        logger.info(f"üìã [HYBRID] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑. –°—Ç–∞–¥–∏—è: {stage}, check_tu: {check_tu}")

        # ============================================================
        # –≠–¢–ê–ü 1: –ü–∞—Ä—Å–∏–Ω–≥ –¢–ó/–¢–£ (—Ä—É—á–Ω–æ–π, –±—ã—Å—Ç—Ä—ã–π, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π)
        # ============================================================

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        files_to_check = [tz_document, doc_document]
        if tu_document:
            files_to_check.append(tu_document)

        for file in files_to_check:
            file_size = await _get_file_size(file)
            if file_size > MAX_FILE_SIZE_BYTES:
                raise HTTPException(
                    status_code=413,
                    detail=f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file_size / 1024 / 1024:.2f} MB). –ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB} MB"
                )

        # Read contents
        await tz_document.seek(0)
        await doc_document.seek(0)
        if tu_document:
            await tu_document.seek(0)

        tz_content = await tz_document.read()
        doc_content = await doc_document.read()
        tu_content = None
        if tu_document:
            tu_content = await tu_document.read()

        logger.info(f"üìä File sizes - TZ: {len(tz_content) / 1024:.1f} KB, DOC: {len(doc_content) / 1024:.1f} KB")

        # Extract TZ text (—Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥)
        logger.info("üìÑ [STEP 1/4] Extracting text from TZ...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected during TZ extraction")
            return AnalysisResponse(
                stage=stage,
                req_type="–¢–ó+–¢–£" if check_tu else "–¢–ó",
                requirements=[],
                summary="–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω: –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¢–ó"
            )

        tz_text = await extract_text_from_pdf(tz_content, tz_document.filename)

        # Handle TU if needed
        has_tu = check_tu and (tu_content is not None or stage in TU_PROMPTS)
        if has_tu:
            logger.info("üìÑ Adding TU to requirements...")
            tu_text = await extract_text_from_pdf(tu_content, tu_document.filename) if tu_content else TU_PROMPTS.get(stage, "")
            tz_text += "\n\n=== –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è (–¢–£) ===\n" + tu_text

        # Segment requirements (–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è)
        logger.info("‚úÇÔ∏è [STEP 2/4] Segmenting requirements from TZ/TU...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected during segmentation")
            return AnalysisResponse(
                stage=stage,
                req_type="–¢–ó+–¢–£" if has_tu else "–¢–ó",
                requirements=[],
                summary="–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω: –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"
            )

        requirements = await segment_requirements(tz_text)

        if not requirements:
            raise HTTPException(status_code=400, detail="No requirements extracted from TZ")

        logger.info(f"‚úÖ Extracted {len(requirements)} requirements")

        # ============================================================
        # –≠–¢–ê–ü 2 [STAGE 1]: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        # ============================================================

        logger.info("üìã [STEP 3/7] STAGE 1: Extracting page metadata...")
        pages_metadata = await extract_page_metadata(doc_content, doc_document.filename, max_pages=150)

        # ============================================================
        # –≠–¢–ê–ü 3 [STAGE 2]: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        # ============================================================

        logger.info("üì§ [STEP 4/7] STAGE 2: Converting to low-res and assessing relevance...")
        doc_images_low = await extract_pdf_pages_as_images(
            doc_content, doc_document.filename,
            max_pages=STAGE2_MAX_PAGES, detail=STAGE2_DETAIL, dpi=STAGE2_DPI, quality=STAGE2_QUALITY
        )

        page_mapping = await assess_page_relevance(pages_metadata, doc_images_low, requirements)

        # ============================================================
        # –≠–¢–ê–ü 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ system prompt
        # ============================================================

        system_prompt = get_analysis_system_prompt(stage, "–¢–ó+–¢–£" if has_tu else "–¢–ó")

        # ============================================================
        # –≠–¢–ê–ü 5 [STAGE 3]: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Å –≤—ã—Å–æ–∫–∏–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º
        # ============================================================

        logger.info(f"üîç [STEP 5/7] STAGE 3: Analyzing with high-resolution images...")
        analyzed_reqs = []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ –æ–±—â–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        from collections import defaultdict
        page_to_reqs = defaultdict(list)

        for req in requirements:
            req_pages = page_mapping.get(req['number'], [])
            if not req_pages:  # Fallback - –ø–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–∞–Ω–∏—Ü
                req_pages = list(range(1, min(21, len(doc_images_low) + 1)))

            pages_key = tuple(sorted(req_pages))
            page_to_reqs[pages_key].append(req)

        logger.info(f"üì¶ [STAGE 3] –°–æ–∑–¥–∞–Ω–æ {len(page_to_reqs)} –≥—Ä—É–ø–ø –ø–æ –æ–±—â–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º")

        for group_idx, (pages_key, reqs_group) in enumerate(page_to_reqs.items(), 1):
            if await request.is_disconnected():
                logger.warning(f"‚ö†Ô∏è Client disconnected at group {group_idx}/{len(page_to_reqs)}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                return AnalysisResponse(
                    stage=stage,
                    req_type="–¢–ó+–¢–£" if has_tu else "–¢–ó",
                    requirements=analyzed_reqs,
                    summary=f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω: –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(analyzed_reqs)}/{len(requirements)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π (–≥—Ä—É–ø–ø–∞ {group_idx}/{len(page_to_reqs)})"
                )

            logger.info(f"üì¶ [STAGE 3] [{group_idx}/{len(page_to_reqs)}] Analyzing {len(reqs_group)} requirements on {len(pages_key)} pages")

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞–∫–µ—Ç—ã –ø–æ N —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –±–æ–ª—å—à–∞—è
            for batch_start in range(0, len(reqs_group), STAGE3_BATCH_SIZE):
                batch = reqs_group[batch_start:batch_start + STAGE3_BATCH_SIZE]

                batch_results = await analyze_batch_with_high_detail(
                    system_prompt=system_prompt,
                    doc_content=doc_content,
                    page_numbers=list(pages_key),
                    requirements_batch=batch,
                    request=request
                )

                if not batch_results:
                    # –ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è batch –∞–Ω–∞–ª–∏–∑–∞
                    return AnalysisResponse(
                        stage=stage,
                        req_type="–¢–ó+–¢–£" if has_tu else "–¢–ó",
                        requirements=analyzed_reqs,
                        summary=f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω: –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è batch –∞–Ω–∞–ª–∏–∑–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(analyzed_reqs)}/{len(requirements)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"
                    )

                analyzed_reqs.extend(batch_results)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É –∏–∑ –¢–ó
        analyzed_reqs.sort(key=lambda r: r.number)

        # ============================================================
        # –≠–¢–ê–ü 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏
        # ============================================================

        logger.info("üìù Generating summary...")
        if await request.is_disconnected():
            logger.warning("‚ö†Ô∏è Client disconnected before summary")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ summary
            return AnalysisResponse(
                stage=stage,
                req_type="–¢–ó+–¢–£" if has_tu else "–¢–ó",
                requirements=analyzed_reqs,
                summary=f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–æ –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–≤–æ–¥–∫–∏. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyzed_reqs)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π."
            )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Å–≤–æ–¥–∫–∏
        total = len(analyzed_reqs)
        completed = sum(1 for r in analyzed_reqs if r.status == "–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ")
        partial = sum(1 for r in analyzed_reqs if r.status == "–ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ")
        not_done = sum(1 for r in analyzed_reqs if r.status == "–ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ")
        unclear = sum(1 for r in analyzed_reqs if r.status == "–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è")

        summary = f"""–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {total}
- –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª–Ω–µ–Ω–æ: {completed} ({completed/total*100:.1f}%)
- –ß–∞—Å—Ç–∏—á–Ω–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ: {partial} ({partial/total*100:.1f}%)
- –ù–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–æ: {not_done} ({not_done/total*100:.1f}%)
- –¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è: {unclear} ({unclear/total*100:.1f}%)

–°—Ä–µ–¥–Ω—è—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {sum(r.confidence for r in analyzed_reqs)/total:.1f}%"""

        # ============================================================
        # –≠–¢–ê–ü 6 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): –ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
        # ============================================================

        if STAGE4_ENABLED:
            logger.info("üîç [STEP 6/7] STAGE 4: –ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
            try:
                contradictions_report = await find_contradictions(
                    pages_metadata=pages_metadata,
                    doc_content=doc_content,
                    requirements=requirements,
                    analyzed_reqs=analyzed_reqs
                )

                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç –æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è—Ö –∫ summary
                summary += "\n\n" + "="*80 + "\n\n" + contradictions_report

                logger.info("‚úÖ [STAGE 4] –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω")

            except Exception as e:
                logger.error(f"‚ùå [STAGE 4] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π: {e}")
                summary += f"\n\n‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω: {str(e)}"
        else:
            logger.info("‚è≠Ô∏è [STAGE 4] –ü—Ä–æ–ø—É—â–µ–Ω (STAGE4_ENABLED=False)")

        # ============================================================
        # –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        # ============================================================

        parsed_result = AnalysisResponse(
            stage=stage,
            req_type="–¢–ó+–¢–£" if has_tu else "–¢–ó",
            requirements=analyzed_reqs,
            summary=summary
        )

        logger.info(f"‚úÖ [HYBRID] –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyzed_reqs)} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.")
        return parsed_result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [HYBRID] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


# ============================
# –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê
# ============================

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
