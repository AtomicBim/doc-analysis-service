"""
FastAPI сервис для анализа документации с использованием OpenAI API.
Использует Assistants API с File Search для обработки строительных чертежей в PDF.
"""
import os
import json
import logging
import asyncio
import warnings
from typing import List, Optional, Dict, Any
from pathlib import Path
import fitz  # pymupdf
from PIL import Image
import io
from tenacity import retry, stop_after_attempt, wait_exponential
import base64

# Отключаем warnings о deprecation Assistants API
warnings.filterwarnings("ignore", category=DeprecationWarning, module="openai")

import uvicorn
from fastapi import FastAPI, HTTPException, Form, UploadFile, File, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError
from openai import AsyncOpenAI
from dotenv import load_dotenv

# ============================
# КОНФИГУРАЦИЯ
# ============================

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Загрузка переменных окружения
load_dotenv()

# Инициализация OpenAI API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY не установлен в переменных окружения!")
    raise ValueError("OPENAI_API_KEY is required")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.1"))
MAX_FILE_SIZE_MB = 40
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
logger.info(f"Используется OpenAI API (Hybrid): {OPENAI_MODEL}")
logger.info("Архитектура: ТЗ/ТУ парсинг вручную + Чертежи через Assistants API")


# ============================
# СИСТЕМА ПРОМПТОВ
# ============================

def load_prompts() -> Dict[str, str]:
    """Загружает промпты из файлов в папке prompts."""
    prompts = {}
    prompts_dir = Path(__file__).parent.parent / "prompts"
    
    # Маппинг стадий на файлы
    stage_files = {
        "ГК": "gk_prompt.txt",
        "ФЭ": "fe_prompt.txt", 
        "ЭП": "ep_prompt.txt"
    }
    
    for stage, filename in stage_files.items():
        file_path = prompts_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                prompts[stage] = f.read().strip()
            logger.info(f"Загружен промпт для стадии {stage} из {file_path}")
        except FileNotFoundError:
            logger.error(f"Не найден файл промпта: {file_path}")
            raise FileNotFoundError(f"Файл промпта не найден: {file_path}")
        except Exception as e:
            logger.error(f"Ошибка загрузки промпта {stage}: {e}")
            raise
    
    return prompts


def load_tu_prompts() -> Dict[str, str]:
    """Загружает предзагруженные ТУ для стадий ФЭ и ЭП."""
    tu_prompts: Dict[str, str] = {}
    prompts_dir = Path(__file__).parent.parent / "prompts"

    tu_stage_files = {
        "ФЭ": "tu_fe.txt",
        "ЭП": "tu_ep.txt",
    }

    for stage, filename in tu_stage_files.items():
        file_path = prompts_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                tu_prompts[stage] = f.read().strip()
            logger.info(f"Загружены предзагруженные ТУ для стадии {stage} из {file_path}")
        except FileNotFoundError:
            logger.error(f"Не найден файл предзагруженных ТУ: {file_path}")
            raise FileNotFoundError(f"Файл предзагруженных ТУ не найден: {file_path}")
        except Exception as e:
            logger.error(f"Ошибка загрузки ТУ {stage}: {e}")
            raise

    return tu_prompts

# Загружаем промпты при инициализации
PROMPTS = load_prompts()
TU_PROMPTS = load_tu_prompts()

# ============================
# ASSISTANTS API ФУНКЦИИ
# ============================

async def upload_to_vector_store(doc_content: bytes, filename: str) -> str:
    """
    Загружает PDF чертежей в Vector Store для File Search.
    Возвращает vector_store_id.
    """
    logger.info(f"Создание Vector Store для {filename}...")

    # Создаем Vector Store
    vector_store = await client.beta.vector_stores.create(
        name=f"Project Documentation - {filename}",
        expires_after={"anchor": "last_active_at", "days": 1}  # Автоудаление через 1 день
    )

    # Сохраняем временный файл
    temp_file_path = f"/tmp/{filename}"
    with open(temp_file_path, 'wb') as f:
        f.write(doc_content)

    # Загружаем файл в Vector Store
    with open(temp_file_path, 'rb') as f:
        file_batch = await client.beta.vector_stores.file_batches.upload_and_poll(
            vector_store_id=vector_store.id,
            files=[f]
        )

    logger.info(f"Vector Store создан: {vector_store.id}, статус: {file_batch.status}")

    # Удаляем временный файл
    os.remove(temp_file_path)

    return vector_store.id


async def create_analysis_assistant(stage: str, req_type: str) -> str:
    """
    Создает Assistant для анализа документации.
    Возвращает assistant_id.
    """
    stage_prompt = PROMPTS.get(stage, PROMPTS["ФЭ"])

    instructions = f"""{stage_prompt}

Ты — эксперт по анализу строительной документации. Твоя задача:

1. Получить требование из {req_type}
2. Найти в проектной документации (чертежах) решение этого требования
3. Вернуть анализ в JSON формате:

{{
  "number": <номер требования>,
  "requirement": "<текст требования>",
  "status": "<Полностью исполнено|Частично исполнено|Не исполнено|Требует уточнения>",
  "confidence": <0-100>,
  "solution_description": "<краткое описание как реализовано>",
  "reference": "<конкретная ссылка: номер листа, раздел, страница>",
  "discrepancies": "<несоответствия или '-'>",
  "recommendations": "<рекомендации или '-'>"
}}

ВАЖНО:
- Используй File Search для поиска релевантных частей чертежей
- Указывай конкретные ссылки (номера листов, разделы)
- Анализируй как текст, так и графические элементы на чертежах
- Если не нашел информацию, указывай status="Требует уточнения"
"""

    assistant = await client.beta.assistants.create(
        name=f"Document Analyzer - {stage}",
        instructions=instructions,
        model=OPENAI_MODEL,
        tools=[{"type": "file_search"}],
        temperature=TEMPERATURE
    )

    logger.info(f"Assistant создан: {assistant.id}")
    return assistant.id


async def analyze_requirement_with_assistant(
    assistant_id: str,
    vector_store_id: str,
    requirement: Dict[str, Any],
    request: Request
) -> Optional[RequirementAnalysis]:
    """
    Анализирует одно требование через Assistants API с File Search.
    Возвращает RequirementAnalysis или None при отключении клиента.
    """
    if await request.is_disconnected():
        logger.warning(f"Client disconnected before analyzing {requirement['trace_id']}")
        return None

    logger.info(f"Анализ требования {requirement['trace_id']}...")

    # Создаем Thread
    thread = await client.beta.threads.create()

    # Отправляем требование
    await client.beta.threads.messages.create(
        thread_id=thread.id,
        role="user",
        content=f"""Проанализируй следующее требование из ТЗ:

Номер: {requirement.get('number')}
Раздел: {requirement.get('section', 'Общие требования')}
Требование: {requirement['text']}

Найди в проектной документации (чертежах), как это требование выполнено.
Верни результат СТРОГО в JSON формате."""
    )

    # Запускаем Assistant с File Search
    run = await client.beta.threads.runs.create_and_poll(
        thread_id=thread.id,
        assistant_id=assistant_id,
        tool_resources={"file_search": {"vector_store_ids": [vector_store_id]}},
        timeout=300  # 5 минут на анализ одного требования
    )

    if run.status != 'completed':
        logger.error(f"Run failed for {requirement['trace_id']}: {run.status}")
        return RequirementAnalysis(
            number=requirement.get('number', 0),
            requirement=requirement['text'],
            status="Требует уточнения",
            confidence=0,
            solution_description="Ошибка анализа",
            reference="-",
            discrepancies=f"Assistant run status: {run.status}",
            recommendations="Повторите анализ",
            section=requirement.get('section'),
            trace_id=requirement['trace_id']
        )

    # Получаем ответ
    messages = await client.beta.threads.messages.list(thread_id=thread.id, order="desc", limit=1)
    assistant_message = messages.data[0]

    # Извлекаем текст ответа
    response_text = ""
    for content_block in assistant_message.content:
        if content_block.type == 'text':
            response_text += content_block.text.value

    # Извлекаем citations для ссылок
    citations = []
    if hasattr(assistant_message.content[0], 'text') and hasattr(assistant_message.content[0].text, 'annotations'):
        for annotation in assistant_message.content[0].text.annotations:
            if hasattr(annotation, 'file_citation'):
                citations.append({
                    'file_id': annotation.file_citation.file_id,
                    'quote': annotation.file_citation.quote
                })

    # Парсим JSON из ответа
    try:
        # Ищем JSON в ответе
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            data = json.loads(json_str)

            # Добавляем citations в reference если есть
            if citations:
                citation_text = "; ".join([f"Цитата: {c['quote'][:100]}..." for c in citations[:2]])
                data['reference'] = f"{data.get('reference', '-')} | {citation_text}"

            return RequirementAnalysis(**data, section=requirement.get('section'), trace_id=requirement['trace_id'])
        else:
            raise ValueError("No JSON found in response")
    except (json.JSONDecodeError, ValidationError) as e:
        logger.error(f"Failed to parse assistant response for {requirement['trace_id']}: {e}")
        logger.error(f"Response was: {response_text[:500]}")
        return RequirementAnalysis(
            number=requirement.get('number', 0),
            requirement=requirement['text'],
            status="Требует уточнения",
            confidence=50,
            solution_description=response_text[:200] if response_text else "Не удалось получить ответ",
            reference="; ".join([c['quote'][:50] for c in citations]) if citations else "-",
            discrepancies="Ошибка парсинга ответа ассистента",
            recommendations="Проверьте вручную",
            section=requirement.get('section'),
            trace_id=requirement['trace_id']
        )


async def cleanup_assistant_resources(assistant_id: Optional[str], vector_store_id: Optional[str]):
    """Удаляет временные ресурсы Assistants API."""
    try:
        if assistant_id:
            await client.beta.assistants.delete(assistant_id)
            logger.info(f"Assistant удален: {assistant_id}")
        if vector_store_id:
            await client.beta.vector_stores.delete(vector_store_id)
            logger.info(f"Vector Store удален: {vector_store_id}")
    except Exception as e:
        logger.error(f"Ошибка при очистке ресурсов: {e}")


# ============================
# PYDANTIC МОДЕЛИ
# ============================

class RequirementAnalysis(BaseModel):
    """Результат анализа одного требования"""
    number: int
    requirement: str
    status: str
    confidence: int
    solution_description: str
    reference: str
    discrepancies: str
    recommendations: str
    section: Optional[str] = None
    trace_id: Optional[str] = None

class AnalysisResponse(BaseModel):
    """Ответ с результатами анализа"""
    stage: str
    req_type: str
    requirements: List[RequirementAnalysis]
    summary: str

# ============================
# FASTAPI ПРИЛОЖЕНИЕ
# ============================

app = FastAPI(
    title="Document Analysis API",
    description="API для анализа строительной документации с использованием OpenAI",
    version="4.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================
# API ENDPOINTS
# ============================

@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "ok",
        "service": "Document Analysis API",
        "provider": "openai",
        "model": OPENAI_MODEL,
        "max_file_size_mb": MAX_FILE_SIZE_MB
    }

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_documentation(
    request: Request,
    stage: str = Form(...),
    check_tu: bool = Form(False),
    req_type: str = Form("ТЗ"),
    tz_document: UploadFile = File(...),
    doc_document: UploadFile = File(...),
    tu_document: Optional[UploadFile] = File(None)
):
    """
    Основной endpoint для анализа документации.
    Принимает файлы и метаданные в multipart/form-data.
    """
    try:
        # Проверяем, не отключился ли клиент
        if await request.is_disconnected():
            logger.warning("Client disconnected before analysis started. Aborting.")
            raise HTTPException(status_code=499, detail="Client disconnected")
        logger.info(f"Получен запрос на анализ. Стадия: {stage}, check_tu: {check_tu}")

        # Проверка размера файлов
        files_to_check = [tz_document, doc_document]
        if tu_document:
            files_to_check.append(tu_document)

        for file in files_to_check:
            file_size = await _get_file_size(file)
            if file_size > MAX_FILE_SIZE_BYTES:
                raise HTTPException(
                    status_code=413,
                    detail=f"Файл {file.filename} слишком большой ({file_size / 1024 / 1024:.2f} MB). Максимальный размер: {MAX_FILE_SIZE_MB} MB"
                )

        # Read contents (after _get_file_size already read them, need to seek back)
        await tz_document.seek(0)
        await doc_document.seek(0)
        if tu_document:
            await tu_document.seek(0)

        tz_content = await tz_document.read()
        doc_content = await doc_document.read()
        tu_content = None
        if tu_document:
            tu_content = await tu_document.read()

        logger.info(f"File sizes - TZ: {len(tz_content)} bytes, DOC: {len(doc_content)} bytes")

        # Extract TZ text
        logger.info("Extracting text from TZ...")
        if await request.is_disconnected():
            logger.warning("Client disconnected during TZ extraction. Aborting.")
            return {"error": "Client disconnected"}
        tz_text = await extract_text_from_pdf(tz_content, tz_document.filename)

        # Segment requirements
        logger.info("Segmenting requirements...")
        if await request.is_disconnected():
            logger.warning("Client disconnected during requirements segmentation. Aborting.")
            return {"error": "Client disconnected"}
        requirements = await segment_requirements(tz_text)

        if not requirements:
            raise HTTPException(status_code=400, detail="No requirements extracted from TZ")

        # Ingest doc
        logger.info("Ingesting project documentation...")
        if await request.is_disconnected():
            logger.warning("Client disconnected during doc ingestion. Aborting.")
            return {"error": "Client disconnected"}
        doc_pages = await ingest_doc(doc_content, doc_document.filename)

        # TODO: Handle TU similarly if check_tu
        has_tu = check_tu and (tu_content is not None or stage in TU_PROMPTS)
        # For simplicity, append TU text to tz_text if present
        if has_tu:
            tu_text = await extract_text_from_pdf(tu_content, tu_document.filename) if tu_content else TU_PROMPTS.get(stage, "")
            tz_text += "\n\nTU:\n" + tu_text
            requirements = await segment_requirements(tz_text)  # Re-segment with TU

        # For now, placeholder for retrieval and analysis
        # Will replace in next edits
        logger.info("Retrieving relevant pages...")
        if await request.is_disconnected():
            logger.warning("Client disconnected before retrieval. Aborting.")
            return {"error": "Client disconnected"}
        all_relevant_pages = {}
        for req in requirements:
            if await request.is_disconnected():
                logger.warning("Client disconnected during page retrieval. Aborting.")
                return {"error": "Client disconnected"}
            all_relevant_pages[req['trace_id']] = await retrieve_relevant_pages(req['text'], doc_pages)

        logger.info("Analyzing requirements in batches...")
        analyzed_reqs = []
        for i in range(0, len(requirements), BATCH_SIZE):
            if await request.is_disconnected():
                logger.warning(f"Client disconnected during batch {i // BATCH_SIZE + 1}. Aborting.")
                return {"error": "Client disconnected"}
            batch = requirements[i:i + BATCH_SIZE]
            analyzed_batch = await analyze_batch(batch, all_relevant_pages, stage, "ТЗ+ТУ" if has_tu else "ТЗ")
            analyzed_reqs.extend(analyzed_batch)

        # Generate summary
        if await request.is_disconnected():
            logger.warning("Client disconnected before summary generation. Aborting.")
            return {"error": "Client disconnected"}
        summary_prompt = f"Summarize analysis of {len(analyzed_reqs)} requirements: {json.dumps([r.dict() for r in analyzed_reqs])}"
        summary_response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=TEMPERATURE
        )
        summary = summary_response.choices[0].message.content
        
        parsed_result = AnalysisResponse(
            stage=stage,
            req_type="ТЗ+ТУ" if has_tu else "ТЗ",
            requirements=analyzed_reqs,
            summary=summary
        )

        logger.info("Анализ завершен успешно")
        return parsed_result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Ошибка при анализе документации: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Ошибка анализа: {str(e)}")

# ============================
# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ
# ============================

async def _get_file_size(file: UploadFile) -> int:
    """Получает размер файла в байтах."""
    content = await file.read()
    await file.seek(0)  # Возвращаем указатель в начало
    return len(content)

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def extract_text_from_pdf(content: bytes, filename: str) -> str:
    """Extracts text from PDF. Uses OCR if no text layer."""
    text = ""
    doc = fitz.open(stream=content, filetype="pdf")
    is_scanned = True
    for page in doc:
        page_text = page.get_text()
        if page_text.strip():
            is_scanned = False
            text += page_text + "\n\n"
    
    if is_scanned:
        # OCR using OpenAI Vision
        for page_num, page in enumerate(doc):
            pix = page.get_pixmap(dpi=DPI)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr = img_byte_arr.getvalue()
            base64_image = base64.b64encode(img_byte_arr).decode('utf-8')
            
            response = await client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Extract all text from this image accurately, preserving structure and formatting."},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                            }
                        ]
                    }
                ],
                temperature=TEMPERATURE,
            )
            text += response.choices[0].message.content + "\n\n"
    
    doc.close()
    return text.strip()

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def segment_requirements(tz_text: str) -> List[Dict[str, Any]]:
    """Segments TZ text into individual requirements using GPT."""
    prompt = f"""Parse the following TZ text into a list of requirements. Each requirement should have:
- number: integer or string identifier
- text: the full text of the requirement
- section: parent section or category
- trace_id: unique identifier like 'req-{{number}}'

Return STRICTLY as JSON: {{"requirements": [{{"number": ..., "text": ..., "section": ..., "trace_id": ...}}]}}

TZ text:
{tz_text}"""

    response = await client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=TEMPERATURE,
        response_format={"type": "json_object"}
    )
    
    try:
        data = json.loads(response.choices[0].message.content)
        return data.get("requirements", [])
    except json.JSONDecodeError:
        raise ValueError("Failed to parse requirements JSON")

async def ingest_doc(content: bytes, filename: str) -> List[Dict[str, str]]:
    """Ingests doc PDF into list of pages with text and base64 image."""
    if not content:
        raise ValueError(f"Empty content provided for file {filename}")

    pages = []
    doc = fitz.open(stream=content, filetype="pdf")
    for page_num, page in enumerate(doc):
        text = page.get_text()
        pix = page.get_pixmap(dpi=DPI)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Ограничиваем размер изображения для экономии токенов
        MAX_WIDTH = 1024
        if img.width > MAX_WIDTH:
            ratio = MAX_WIDTH / img.width
            new_height = int(img.height * ratio)
            img = img.resize((MAX_WIDTH, new_height), Image.Resampling.LANCZOS)

        # Сжимаем JPEG с качеством 85 вместо PNG
        img_byte_arr = io.BytesIO()
        img.save(img_byte_arr, format='JPEG', quality=85, optimize=True)
        base64_image = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
        pages.append({
            "page_num": page_num + 1,
            "text": text.strip(),
            "image": base64_image
        })
    doc.close()
    return pages

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def get_embedding(text: str) -> List[float]:
    """Gets embedding for text using OpenAI. Truncates if exceeds token limit."""
    # text-embedding-3-small max tokens: 8191
    # Approximate: 1 token ≈ 4 chars, safe limit = 8000 tokens ≈ 32000 chars
    MAX_CHARS = 32000

    if len(text) > MAX_CHARS:
        logger.warning(f"Text exceeds {MAX_CHARS} chars ({len(text)}), truncating for embedding")
        text = text[:MAX_CHARS]

    response = await client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

import numpy as np

async def retrieve_relevant_pages(req_text: str, doc_pages: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Retrieves top-k relevant pages using embedding similarity."""
    if not doc_pages:
        return []

    req_emb = await get_embedding(req_text)
    page_embs = []
    for page in doc_pages:
        page_text = page['text'].strip()
        if page_text:
            page_embs.append(await get_embedding(page_text))
        else:
            page_embs.append([0] * len(req_emb))  # Zero vector for empty text

    similarities = [np.dot(req_emb, p_emb) / (np.linalg.norm(req_emb) * np.linalg.norm(p_emb) + 1e-8) for p_emb in page_embs]
    top_indices = np.argsort(similarities)[-TOP_K:][::-1]
    return [doc_pages[i] for i in top_indices]

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def analyze_batch(batch: List[Dict[str, Any]], all_relevant_pages: Dict[str, List[Dict[str, str]]], stage: str, req_type: str) -> List[RequirementAnalysis]:
    """Analyzes batch of requirements with multimodal context."""
    stage_prompt = PROMPTS.get(stage, PROMPTS["ФЭ"])

    messages = [{"role": "system", "content": stage_prompt}]

    content = [{"type": "text", "text": f"Analyze these requirements against the documentation. Return STRICTLY JSON as {AnalysisResponse.model_json_schema()} but only the requirements array."}]

    for req in batch:
        content.append({"type": "text", "text": f"Requirement {req['trace_id']}: {req['text']}"})
        for page in all_relevant_pages.get(req['trace_id'], []):
            # Добавляем текст страницы
            if page['text']:
                content.append({"type": "text", "text": f"Page {page['page_num']} text: {page['text'][:5000]}"})  # Ограничиваем до 5000 символов
            # Добавляем изображение только если текст короткий или отсутствует
            if len(page['text']) < 500:
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{page['image']}"}
                })

    messages.append({"role": "user", "content": content})

    # Логируем размер контекста для отладки
    total_text_chars = sum(len(item.get('text', '')) for item in content if item.get('type') == 'text')
    total_images = sum(1 for item in content if item.get('type') == 'image_url')
    logger.info(f"Batch analysis: {len(batch)} requirements, {total_text_chars} text chars, {total_images} images")

    response = await client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages,
        temperature=TEMPERATURE,
        response_format={"type": "json_object"}
    )
    
    try:
        data = json.loads(response.choices[0].message.content)
        return [RequirementAnalysis(**item) for item in data.get("requirements", [])]
    except (json.JSONDecodeError, ValidationError) as e:
        logger.error(f"Invalid JSON or validation error: {e}")
        raise


def _parse_analysis_response(response_text: str, stage: str, req_type: str) -> AnalysisResponse:
    """Парсинг JSON ответа от AI в Pydantic модель."""
    try:
        # Убираем возможные markdown блоки
        cleaned_response = response_text.strip()
        if cleaned_response.startswith("```"):
            # Находим JSON между ```json и ```
            start = cleaned_response.find("{")
            end = cleaned_response.rfind("}") + 1
            if start != -1 and end > start:
                cleaned_response = cleaned_response[start:end]

        data = json.loads(cleaned_response)

        requirements = [RequirementAnalysis(**req) for req in data.get("requirements", [])]

        return AnalysisResponse(
            stage=stage,
            req_type=req_type,
            requirements=requirements,
            summary=data.get("summary", "Анализ завершен")
        )
    except json.JSONDecodeError as e:
        logger.error(f"Ошибка парсинга JSON: {e}. Ответ AI: {response_text}")
        raise ValueError(f"Не удалось обработать ответ от AI. Ошибка парсинга JSON: {str(e)}")


# ============================
# ЗАПУСК СЕРВЕРА
# ============================

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
