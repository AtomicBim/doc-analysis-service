# 🔍 STAGE 2: Алгоритм оценки релевантности страниц

## 📍 Общая информация

**Когда:** Вторая стадия анализа, после извлечения метаданных (STAGE 1)  
**Где:** Функция `assess_page_relevance()` в `api-service/analysis_api_hybrid.py` (строки 373-502)  
**Цель:** Определить минимальный набор релевантных страниц для каждого требования  

**Входные данные:**
- `pages_metadata` - метаданные с STAGE 1 (названия, разделы, номера листов)
- `requirements` - список требований из ТЗ
- `doc_content` - содержимое PDF документа

**Выходные данные:**
```python
{
    1: [5, 12, 18],      # Требование 1 → страницы 5, 12, 18
    2: [22, 23, 45],     # Требование 2 → страницы 22, 23, 45
    3: [18, 33, 34, 35]  # Требование 3 → страницы 18, 33, 34, 35
}
```

---

## 🎯 Пошаговый алгоритм

### **ФАЗА 1: Текстовый префильтр (быстрый отбор кандидатов)** ⚡

**Цель:** Сократить 150+ страниц до ~30 страниц-кандидатов **БЕЗ использования Vision API**

#### **Шаг 1.1: Быстрое извлечение текста**

```python
def _extract_page_texts_quick(doc_content: bytes, max_pages: int = 200) -> List[str]:
    """Извлекает текст БЕЗ OCR - только встроенный текстовый слой PDF"""
    texts: List[str] = []
    doc = fitz.open(stream=doc_content, filetype="pdf")
    
    for i in range(min(len(doc), max_pages)):
        page = doc[i]
        page_text = page.get_text() or ""  # Быстро! Без Vision API
        texts.append(page_text)
    
    return texts
```

**⚡ Производительность:** 100-200 страниц за ~1-2 секунды  
**💰 Стоимость:** 0 токенов (локальная обработка)

---

#### **Шаг 1.2: Подсчет совпадений ключевых слов**

Алгоритм ранжирования страниц по совпадению токенов:

```python
def _simple_candidate_pages(
    requirements: List[Dict], 
    page_texts: List[str], 
    per_req: int = 7,        # Top-7 страниц на требование
    cap_total: int = 30      # Макс. 30 уникальных страниц
) -> List[int]:
    
    # 1. Токенизация (простой regex)
    def tokenize(text: str) -> List[str]:
        return re.findall(r"[A-Za-zА-Яа-яЁё0-9_-]{2,}", text.lower())
    
    page_tokens = [tokenize(text) for text in page_texts]
    
    candidates = []
    
    # 2. Для каждого требования
    for req in requirements:
        req_words = set(tokenize(req['text']))  # Уникальные токены требования
        
        scores = []
        
        # 3. Подсчет совпадений для каждой страницы
        for page_idx, page_toks in enumerate(page_tokens):
            # Количество совпадающих слов
            score = sum(1 for word in page_toks if word in req_words)
            
            if score > 0:
                scores.append((page_idx, score))
        
        # 4. Сортировка по убыванию score
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # 5. Берем top-7 страниц для этого требования
        top_pages = [idx for (idx, _) in scores[:per_req]]
        candidates.extend(top_pages)
    
    # 6. Убираем дубликаты, сортируем, ограничиваем 30 страницами
    unique_pages = sorted(list(set(candidates)))[:cap_total]
    
    return [page + 1 for page in unique_pages]  # Конвертируем в 1-based
```

---

#### **Пример работы текстового префильтра:**

```
📋 ТРЕБОВАНИЕ 1: "Высота потолков 2.64м в жилых помещениях"
   Токены: ["высота", "потолков", "264м", "жилых", "помещениях"]

📄 СТРАНИЦА 5 (План 1 этажа):
   Текст: "План 1 этажа. Высота потолков 2.64. Жилые помещения..."
   Токены: [..., "высота", "потолков", "264", "жилые", "помещения", ...]
   Совпадений: 5 ⭐⭐⭐⭐⭐
   
📄 СТРАНИЦА 12 (Разрез 1-1):
   Текст: "Разрез 1-1. Отм. 0.000. Отм. +2.640. Потолок жилого этажа..."
   Токены: [..., "2640", "потолок", "жилого", ...]
   Совпадений: 2 ⭐⭐
   
📄 СТРАНИЦА 33 (Схема ОВ):
   Текст: "Схема вентиляции. Приточная установка..."
   Совпадений: 0

🎯 РЕЗУЛЬТАТ для Требования 1:
   Top-7 страниц: [5, 12, 18, 22, 45, 67, 89]
```

---

#### **Итог ФАЗЫ 1:**

```
150 страниц PDF
    ↓ (текстовый анализ без Vision API)
~30 страниц-кандидатов

Экономия: 120 страниц × 765 токенов = ~91,800 токенов НЕ отправлено в Vision API
```

---

### **ФАЗА 2: Конвертация кандидатов в изображения** 🖼️

#### **Шаг 2.1: Извлечение ТОЛЬКО отобранных страниц**

```python
# Извлекаем только выбранные страницы в ВЫСОКОМ качестве
doc_images_low, page_numbers_kept = await extract_selected_pdf_pages_as_images(
    doc_content, 
    filename,
    selected_pages=[5, 12, 18, 22, ...],  # Только кандидаты!
    detail="high",     # gpt-5-mini дешевая, используем high
    dpi=120,           # Хорошее качество
    quality=85         # Высокое JPEG качество
)
```

**Параметры качества (из `config.py`):**
```python
STAGE2_DPI = 120               # Повышено для точности
STAGE2_QUALITY = 85            # Высокое JPEG качество
STAGE2_DETAIL = "high"         # 765 токенов на изображение
STAGE2_MAX_PAGES = 100         # Максимум страниц для обработки
STAGE2_MAX_PAGES_PER_REQUEST = 30  # Батчинг для избежания rate limit
```

**Результат:** 30 base64-encoded изображений высокого качества

---

### **ФАЗА 3: Vision API анализ релевантности** 🤖

#### **Шаг 3.1: Подготовка данных для модели**

Формируется описание страниц из метаданных STAGE 1:

```python
pages_description = "\n".join([
    f"Страница {p['page']}: {p.get('title', 'N/A')} [{p.get('section', 'N/A')}] - {p.get('type', 'N/A')}"
    for p in pages_metadata
])
```

**Пример:**
```
Страница 5: План 1 этажа [АР] - план
Страница 12: Разрез 1-1 [АР] - разрез
Страница 18: Схема вентиляции [ОВ] - схема
Страница 22: План кровли [АР] - план
...
```

Формируется список требований (краткая версия):

```python
requirements_text = "\n".join([
    f"{req['number']}. [{req.get('section', 'Общие')}] {req['text'][:200]}..."
    for req in requirements
])
```

**Пример:**
```
1. [Общие] Высота потолков в жилых помещениях должна составлять 2.64м...
2. [ОВ] Предусмотреть приточно-вытяжную вентиляцию с рекуперацией тепла...
3. [АР] Паркинг двухуровневый на 50 машиномест...
```

---

#### **Шаг 3.2: Формирование запроса к Vision API**

```python
content = [
    {
        "type": "text",
        "text": STAGE_PROMPTS["stage2_relevance"].format(
            page_count=30,
            pages_description=pages_description,
            requirements_text=requirements_text
        )
    },
    
    # Добавляем изображения
    {"type": "text", "text": "\n--- Страница 5 ---"},
    {"type": "image_url", "image_url": {
        "url": "data:image/jpeg;base64,...", 
        "detail": "high"  # ← 765 токенов
    }},
    
    {"type": "text", "text": "\n--- Страница 12 ---"},
    {"type": "image_url", "image_url": {
        "url": "data:image/jpeg;base64,...",
        "detail": "high"
    }},
    
    # ... и так далее для всех 30 страниц
]
```

**Запрос к API:**
```python
response = await client.chat.completions.create(
    model="gpt-5-mini",           # Дешевая модель для релевантности
    messages=[{"role": "user", "content": content}],
    response_format={"type": "json_object"},
    max_completion_tokens=4000
)
```

---

#### **Шаг 3.3: Промпт для модели**

Модель получает промпт из `prompts/stage2_page_relevance_prompt.txt`:

```
Ты эксперт по строительной документации.

Перед тобой 30 страниц проектной документации в ВЫСОКОМ разрешении.
Твоя задача: ТОЧНО определить минимальный набор страниц для каждого требования.

ПРИНЦИПЫ ОТБОРА:
1. Включай ТОЛЬКО страницы с ПРЯМОЙ релевантностью
2. Оптимально: 3-7 страниц на требование (не более 10)
3. Предпочитай конкретные данные над общими
4. Исключай дубли и повторения
5. Для комплексных требований - ключевые листы из каждого раздела

ПРИМЕРЫ:
- "Высота потолков 2.64м" → планы этажей + разрезы (3-5 листов АР)
- "Паркинг 2-уровневый" → генплан + планы паркинга (2-4 листа)

Верни JSON:
{
  "page_mapping": [
    {"requirement_number": 1, "relevant_pages": [5, 12, 18], "reason": "..."},
    {"requirement_number": 2, "relevant_pages": [22, 23], "reason": "..."}
  ]
}
```

---

#### **Шаг 3.4: Ответ модели**

Модель анализирует изображения и возвращает:

```json
{
  "page_mapping": [
    {
      "requirement_number": 1,
      "relevant_pages": [5, 12, 18],
      "reason": "План 1 этажа с высотами, Разрез 1-1 с отметками, Спецификация высот"
    },
    {
      "requirement_number": 2,
      "relevant_pages": [22, 23, 45],
      "reason": "Схема приточной вентиляции, Схема вытяжной вентиляции, Спецификация оборудования ОВ"
    },
    {
      "requirement_number": 3,
      "relevant_pages": [67, 68, 69, 70],
      "reason": "Генплан с паркингом, План паркинга -1 уровень, План паркинга -2 уровень, Разрез паркинга"
    }
  ]
}
```

---

### **ФАЗА 4: Обработка батчей (для больших документов)** 📦

Если страниц-кандидатов больше 30:

```python
if len(doc_images_low) > STAGE2_MAX_PAGES_PER_REQUEST:
    # Разбиваем на батчи по 30 страниц
    all_page_mappings = []
    
    for batch_start in range(0, len(doc_images_low), 30):
        batch_end = min(batch_start + 30, len(doc_images_low))
        batch_images = doc_images_low[batch_start:batch_end]
        
        # Анализируем батч
        batch_mapping = await _analyze_relevance_batch(
            batch_metadata, 
            batch_images, 
            requirements
        )
        
        all_page_mappings.append(batch_mapping)
    
    # Объединяем результаты всех батчей
    combined_mapping = {}
    for req in requirements:
        req_num = req['number']
        combined_pages = []
        
        # Собираем страницы из всех батчей
        for batch_mapping in all_page_mappings:
            if req_num in batch_mapping:
                combined_pages.extend(batch_mapping[req_num])
        
        # Убираем дубликаты и сортируем
        combined_mapping[req_num] = sorted(list(set(combined_pages)))
```

**Пример:**
```
60 страниц-кандидатов
    ↓
Батч 1: страницы 1-30   → {1: [5, 12], 2: [22, 23]}
Батч 2: страницы 31-60  → {1: [45], 2: [50, 55]}
    ↓
Объединение: {1: [5, 12, 45], 2: [22, 23, 50, 55]}
```

---

## 📊 Визуализация полного процесса

```
┌─────────────────────────────────────────────────────────┐
│  ВХОДНЫЕ ДАННЫЕ                                         │
├─────────────────────────────────────────────────────────┤
│  • 150 страниц PDF документа                            │
│  • 25 требований из ТЗ                                  │
│  • Метаданные страниц (STAGE 1)                         │
└─────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────┐
│  ФАЗА 1: ТЕКСТОВЫЙ ПРЕФИЛЬТР ⚡                          │
├─────────────────────────────────────────────────────────┤
│  1. Быстрое извлечение текста (без OCR)                │
│     150 страниц → 150 текстов за ~2 сек                 │
│                                                          │
│  2. Подсчет совпадений ключевых слов                    │
│     Для каждого требования:                             │
│     • Токенизация текста требования                     │
│     • Подсчет совпадений на каждой странице             │
│     • Ранжирование страниц по score                     │
│     • Выбор top-7 страниц                               │
│                                                          │
│  3. Объединение и де-дупликация                         │
│     25 требований × 7 страниц = 175 кандидатов          │
│     Уникальных: ~30 страниц                             │
│                                                          │
│  💰 Экономия: ~91,800 токенов (120 страниц не отправлены)│
└─────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────┐
│  ФАЗА 2: КОНВЕРТАЦИЯ В ИЗОБРАЖЕНИЯ 🖼️                   │
├─────────────────────────────────────────────────────────┤
│  Параметры:                                             │
│  • DPI: 120 (высокое качество)                          │
│  • JPEG quality: 85                                     │
│  • Detail: "high" (765 токенов/изображение)            │
│                                                          │
│  30 страниц → 30 base64 изображений                     │
│  Размер: ~2-3 MB (сжато в JPEG)                         │
└─────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────┐
│  ФАЗА 3: VISION API АНАЛИЗ 🤖                           │
├─────────────────────────────────────────────────────────┤
│  1. Подготовка промпта:                                 │
│     • Метаданные страниц (названия, разделы)            │
│     • Список требований (краткий)                       │
│     • 30 изображений высокого качества                  │
│                                                          │
│  2. Запрос к gpt-5-mini:                                │
│     Input: ~23,000 токенов (30 × 765 + промпт)          │
│     Output: ~2,000 токенов (JSON mapping)               │
│                                                          │
│  3. Парсинг ответа:                                     │
│     {                                                    │
│       1: [5, 12, 18],     # Требование 1 → 3 страницы   │
│       2: [22, 23, 45],    # Требование 2 → 3 страницы   │
│       ...                                                │
│     }                                                    │
└─────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────┐
│  ФАЗА 4: БАТЧИНГ (если > 30 страниц)                   │
├─────────────────────────────────────────────────────────┤
│  Разбиение на батчи по 30 страниц                       │
│  Объединение результатов                                │
│  Удаление дубликатов                                    │
└─────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────┐
│  ВЫХОДНЫЕ ДАННЫЕ                                        │
├─────────────────────────────────────────────────────────┤
│  page_mapping = {                                       │
│      1: [5, 12, 18],           # 3 релевантные страницы │
│      2: [22, 23, 45],          # 3 релевантные страницы │
│      3: [18, 33, 34, 35],      # 4 релевантные страницы │
│      ...                                                 │
│  }                                                       │
│                                                          │
│  📊 Статистика:                                         │
│  • 25 требований                                        │
│  • ~3-7 страниц на требование                           │
│  • Общий пул: ~40-60 уникальных страниц                 │
│                                                          │
│  → Эти страницы будут анализироваться в STAGE 3         │
└─────────────────────────────────────────────────────────┘
```

---

## 💡 Примеры работы алгоритма

### **Пример 1: Простое требование**

**Требование #1:**
```
"Высота потолков в жилых помещениях должна составлять 2.64м"
```

**Текстовый префильтр (ФАЗА 1):**
```
Кандидаты: [5, 12, 18, 22, 45, 67, 89]
(страницы с упоминаниями "высота", "потолк", "264", "жилых")
```

**Vision API анализ (ФАЗА 3):**
```json
{
  "requirement_number": 1,
  "relevant_pages": [5, 12, 18],
  "reason": "Стр.5 - План 1 этажа с отметкой высоты 2.64м, Стр.12 - Разрез 1-1 показывает высоту потолка, Стр.18 - Спецификация высот помещений"
}
```

**Результат:** 3 страницы вместо 7 кандидатов ✅

---

### **Пример 2: Комплексное требование**

**Требование #5:**
```
"Организовать двухуровневый подземный паркинг на 50 машиномест с въездом с северной стороны участка, обеспечить вентиляцию паркинга согласно СП"
```

**Текстовый префильтр (ФАЗА 1):**
```
Кандидаты: [1, 2, 33, 34, 35, 50, 51, 52, 78, 79]
(генплан, планы паркинга, вентиляция)
```

**Vision API анализ (ФАЗА 3):**
```json
{
  "requirement_number": 5,
  "relevant_pages": [1, 33, 34, 50, 78],
  "reason": "Стр.1 - Генплан с въездом в паркинг (северная сторона), Стр.33-34 - Планы паркинга -1 и -2 уровня с расстановкой м/мест (50 мест), Стр.50 - Разрез паркинга с отметками, Стр.78 - Схема вентиляции паркинга"
}
```

**Результат:** 5 страниц из разных разделов (ГП, АР, ОВ) ✅

---

### **Пример 3: Требование с отсутствующими данными**

**Требование #12:**
```
"Предусмотреть зарядные станции для электромобилей"
```

**Текстовый префильтр (ФАЗА 1):**
```
Кандидаты: [33, 89, 90]
(паркинг, электроснабжение)
```

**Vision API анализ (ФАЗА 3):**
```json
{
  "requirement_number": 12,
  "relevant_pages": [],
  "reason": "Зарядные станции не обнаружены ни на планах паркинга, ни на схемах электроснабжения"
}
```

**Результат:** 0 страниц → Требование будет помечено "Требует уточнения" в STAGE 3 ⚠️

---

## ⚙️ Технические детали

### **Настройки качества:**

```python
# Высокое качество для точности (gpt-5-mini дешевая)
STAGE2_DPI = 120                   # Качество рендеринга
STAGE2_QUALITY = 85                # JPEG качество
STAGE2_DETAIL = "high"             # 765 токенов/изображение
STAGE2_MAX_PAGES = 100             # Макс. страниц для обработки
STAGE2_MAX_PAGES_PER_REQUEST = 30  # Батчинг (30 × 765 = 22,950 токенов)
```

### **Fallback механизм:**

```python
except Exception as e:
    logger.error(f"❌ [STAGE 2] Ошибка оценки релевантности: {e}")
    # Fallback: все страницы батча для всех требований
    return {
        req['number']: list(range(offset + 1, offset + len(batch_images) + 1)) 
        for req in requirements
    }
```

### **Логирование:**

```python
logger.info(f"📄 [STAGE 2] Текстовый префильтр выбрал: {candidate_pages[:10]}...")
logger.info(f"📄 [STAGE 2] Req {req_num}: страницы {pages} ({reason})")
logger.info(f"✅ [STAGE 2] Построен mapping для {len(page_mapping)} требований")
```

---

## 📈 Оптимизации

### **1. Двухфазный подход**

```
ВМЕСТО:
150 страниц × 765 токенов = 114,750 токенов на Vision API

ИСПОЛЬЗУЕМ:
Фаза 1: Текстовый анализ (0 токенов Vision API)
Фаза 2: 30 страниц × 765 токенов = 22,950 токенов

Экономия: ~80% токенов Vision API
```

### **2. Высокое качество для дешевой модели**

```python
# gpt-5-mini в ~15 раз дешевле gpt-4o
# Можем позволить себе "high" detail
STAGE2_DETAIL = "high"  # 765 токенов вместо 255 (low)
```

**Выгода:** Точность определения релевантности выше, стоимость всё равно низкая

### **3. Батчинг для избежания rate limit**

```python
# Максимум 30 страниц в запросе
# 30 × 765 = 22,950 токенов < 30,000 TPM лимит
STAGE2_MAX_PAGES_PER_REQUEST = 30
```

### **4. Кэширование метаданных**

```python
# Метаданные из STAGE 1 переиспользуются
# Не нужно заново извлекать названия, разделы, типы
pages_description = "\n".join([
    f"Страница {p['page']}: {p.get('title')} [{p.get('section')}] - {p.get('type')}"
    for p in pages_metadata  # ← Уже есть из STAGE 1
])
```

---

## 🎯 Итог

**STAGE 2** - это **интеллектуальный фильтр**, который:

1. ⚡ **Текстовый префильтр:** Быстро сокращает 150 страниц до ~30 кандидатов (без Vision API)
2. 🖼️ **Конвертация:** Превращает кандидатов в высококачественные изображения
3. 🤖 **Vision API:** Точно определяет 3-7 релевантных страниц для каждого требования
4. 📦 **Батчинг:** Обрабатывает большие документы порционно
5. 💾 **Mapping:** Создает связь требование → страницы для STAGE 3

**Эффект:**
- Сокращение данных для STAGE 3: 150 → 40-60 страниц
- Экономия токенов: ~80%
- Точность: высокая (Vision API + метаданные)
- Скорость: быстрая (текстовый префильтр)

**Следующий шаг:** STAGE 3 будет анализировать ТОЛЬКО отобранные релевантные страницы в высоком разрешении! 🚀

